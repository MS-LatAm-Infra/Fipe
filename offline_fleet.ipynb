{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe21c13",
   "metadata": {},
   "source": [
    "# Offline Fleet Processing Notebook\n",
    "\n",
    "Notebook version of `fleet.py` focused on offline processing starting from existing parsed raw CSVs. Follow the sections sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Imports\n",
    "import pandas as pd, numpy as np, re, json, logging, math\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime\n",
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)7s | %(message)s')\n",
    "log = logging.getLogger('offline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14aa1e3",
   "metadata": {},
   "source": [
    "## 2. Path Configuration & Input File Selection\n",
    "Define base directories and auto-detect latest input CSV files. Adjust manually if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e84821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect helper\n",
    "def latest(pattern: str, base: Path) -> Path | None:\n",
    "    files = sorted(base.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "BASE = Path('.')\n",
    "DATA_DIR = BASE / 'data'\n",
    "RAW_DIR = BASE / 'raw'\n",
    "RAW_LOCALIZA = RAW_DIR / 'localiza'\n",
    "RAW_MOVIDA = RAW_DIR / 'movida'\n",
    "FIPE_DIR = DATA_DIR / 'fipe'\n",
    "TUPLES_DIR = DATA_DIR / 'tuples'\n",
    "TABLES_DIR = DATA_DIR / 'tables'\n",
    "for d in [DATA_DIR, RAW_LOCALIZA, RAW_MOVIDA, FIPE_DIR, TUPLES_DIR, TABLES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "localiza_csv = latest('localiza_seminovos_*.csv', RAW_LOCALIZA)\n",
    "movida_csv = latest('movida_seminovos_*.csv', RAW_MOVIDA)\n",
    "fipe_models_csv = (DATA_DIR / 'fipe_models.csv') if (DATA_DIR / 'fipe_models.csv').exists() else Path('fipe_models.csv')\n",
    "if not fipe_models_csv.exists(): fipe_models_csv = None\n",
    "fipe_dump_csv = latest('fipe_dump_*.csv', FIPE_DIR)\n",
    "\n",
    "log.info('Localiza CSV: %s', localiza_csv)\n",
    "log.info('Movida   CSV: %s', movida_csv)\n",
    "log.info('FIPE models CSV: %s', fipe_models_csv)\n",
    "log.info('FIPE dump CSV: %s', fipe_dump_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c204b4",
   "metadata": {},
   "source": [
    "## 3. Utility Functions (Dates, Logging, Helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ymd_compact(d: date | None = None) -> str:\n",
    "    return (d or date.today()).strftime('%Y%m%d')\n",
    "\n",
    "def today_iso() -> str:\n",
    "    return date.today().isoformat()\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clean_price_to_int(x):\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)):\n",
    "        return pd.NA\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        return int(round(float(x)))\n",
    "    s = re.sub(r'[^\\d\\.,]', '', str(x))\n",
    "    if ',' in s and '.' in s:\n",
    "        s = s.replace('.', '').replace(',', '.')\n",
    "    elif ',' in s:\n",
    "        s = s.replace(',', '.')\n",
    "    try:\n",
    "        return int(round(float(s)))\n",
    "    except Exception:\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7222b",
   "metadata": {},
   "source": [
    "## 4. Normalization Helpers (Accents, Text Cleaning, Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e19ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical normalization & helper functions (from fleet.py)\n",
    "import re, unicodedata, pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", str(s))\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def remove_duplicate_words(text: str) -> str:\n",
    "    seen = set(); result=[]\n",
    "    for word in text.split():\n",
    "        key = word.casefold()\n",
    "        if key not in seen:\n",
    "            seen.add(key); result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    import pandas as pd, re\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(s).lower())\n",
    "    s0 = s0.replace(\",\", \".\")\n",
    "    s0 = s0.replace(\"c/ar\", \"\").replace(\"c/ ar\",\"\")\n",
    "    s0 = re.sub(r'/', ' ', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:T\\.)(?!\\w)', 'turbo', s0)\n",
    "    s0 = re.sub(r\"[^a-z0-9\\.\\s]\", \" \", s0)\n",
    "    s0 = re.sub(r\"\\bautomatic[oa]\\b|\\bat\\b|\\baut(?:\\.|o)?\\b\", \"aut\", s0)\n",
    "    s0 = re.sub(r\"\\bman(?:ual)?\\b|\\bmecanico\\b\", \"mec\", s0)\n",
    "    s0 = re.sub(r\"\\bt\\s?si\\b\", \"tsi\", s0)\n",
    "    s0 = re.sub(r\"\\b(\\d{2,4}(?:i|d))\\s*a\\b\", r\"\\1 aut\", s0)\n",
    "    s0 = re.sub(r'(?<=[A-Za-z])\\.(?=[A-Za-z])', '. ', s0)\n",
    "    s0 = re.sub(r'(?<=[A-Za-z])\\.', '', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:perf|perfor|performa|performance|p)(?!\\w)', 'performance', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:long)(?!\\w)', 'longitude', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:sportb|SPB|SB)(?!\\w)', 'sportback', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:prest)(?!\\w)', 'prestige', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:ultim)(?!\\w)', 'ultimate', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:insc)(?!\\w)', 'inscription', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:xdrive30e)(?!\\w)', 'xdrive 30e', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:cp)(?!\\w)', 'cs plus', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:7l)(?!\\w)', '', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:hurric|hurr)(?!\\w)', 'hurricane', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:overl)(?!\\w)', 'overland', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:dies|die)(?!\\w)', 'diesel', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:tb)(?!\\w)', 'turbo', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:sed)(?!\\w)', 'sedan', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:step)(?!\\w)', 'stepway', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:hig)(?!\\w)', 'highline', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:limit)(?!\\w)', 'limited', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:plat)(?!\\w)', 'platinum', s0)\n",
    "    s0 = re.sub(r'(?i)\\b\\d+[pv]\\b', '', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:exclu)(?!\\w)', 'exclusive', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:t270)(?!\\w)', 'turbo 270', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:comfort|comfor)(?!\\w)', 'comfortline', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+HATCH\\s+PREM\\.\\b', 'onix hatch premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SEDAN\\s+PREM\\.\\b', 'onix sedan premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SEDAN\\s+Plus+\\s+PREM\\.\\b', 'onix sedan plus premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SD\\.\\s+P\\.\\s+PR\\.\\b', 'onix sedan plus premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bFastback\\s+Limited+\\s+Ed\\.\\b', 'fastback limited edition', s0)\n",
    "    s0 = re.sub(r'(?i)\\bAIRCROSS\\s+F\\.\\b', 'aircross feel', s0)\n",
    "    s0 = re.sub(r'(?<=xc)(\\d+)', r' \\1', s0)\n",
    "    s0 = re.sub(r'\\bnew\\b(?![\\s-]*(?:range|beetle)\\b)', '', s0, flags=re.IGNORECASE)\n",
    "    s0 = remove_duplicate_words(s0)\n",
    "    s0 = re.sub(r\"\\s+\", \" \", s0).strip()\n",
    "    return s0\n",
    "\n",
    "def generic_norm_text(s: str) -> str:\n",
    "    import re\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(s).lower())\n",
    "    s0 = re.sub(r\"[^a-z0-9\\s]\", \" \", s0)\n",
    "    s0 = re.sub(r'(?<=xc)(\\d+)', r' \\1', s0)\n",
    "    s0 = re.sub(r\"\\s+\", \" \", s0).strip()\n",
    "    return s0\n",
    "\n",
    "def norm_brand(s: str) -> str:\n",
    "    s0 = generic_norm_text(s)\n",
    "    aliases = {\n",
    "        \"vw - volkswagen\":\"volkswagen\",\"vw volkswagen\":\"volkswagen\",\"volks\":\"volkswagen\",\"volkswagem\":\"volkswagen\",\n",
    "        \"gm\":\"chevrolet\",\"gm - chevrolet\":\"chevrolet\",\"gm chevrolet\":\"chevrolet\",\"chevy\":\"chevrolet\",\n",
    "        \"mercedes-benz\":\"mercedes benz\",\"mb\":\"mercedes benz\",\"caoa chery\":\"chery\",\"caoa chery/chery\":\"chery\",\"great wall\":\"gwm\"\n",
    "    }\n",
    "    return aliases.get(s0, s0)\n",
    "\n",
    "def tokset(s: str) -> set:\n",
    "    return set(generic_norm_text(s).split())\n",
    "\n",
    "def extract_engine(s: str):\n",
    "    m = re.search(r\"\\b(\\d\\.\\d)\\b\", str(s))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# Type normalization\n",
    "\n",
    "def normalize_type(raw_type, fuel_sigla):\n",
    "    if fuel_sigla:\n",
    "        fs = str(fuel_sigla).upper().strip()\n",
    "        if fs in {'E','H'}:\n",
    "            return 'ev'\n",
    "    if raw_type is None or (isinstance(raw_type, float) and pd.isna(raw_type)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(raw_type)).upper().strip()\n",
    "    mapping = {\n",
    "        'SEDAN':'SEDAN','SEDÃ':'SEDAN','SEDA':'SEDAN',\n",
    "        'HATCH':'HATCH','HATCHBACK':'HATCH',\n",
    "        'SUV':'SUV',\n",
    "        'PICAPE':'PICKUP/VANS','PICAPE CABINE DUPLA':'PICKUP/VANS','PICK-UP':'PICKUP/VANS','PICKUP':'PICKUP/VANS','CAMINHONETE':'PICKUP/VANS','CABINE SIMPLES':'PICKUP/VANS',\n",
    "        'UTILITÁRIO':'PICKUP/VANS','FURGAO':'PICKUP/VANS','MINIVAN':'PICKUP/VANS','VAN':'PICKUP/VANS','CARGA':'PICKUP/VANS',\n",
    "        'ELETRICO':'EV','HIBRIDO':'EV',\n",
    "        'COUPE':'PREMIUM','SPORTBACK':'PREMIUM','GRAN COUPE':'PREMIUM','FASTBACK':'PREMIUM',\n",
    "        'PARTICULAR':'OTHER','OUTROS':'OTHER','OUTRO':'OTHER'\n",
    "    }\n",
    "    return mapping.get(s0, s0).lower()\n",
    "\n",
    "# Matching scorer (canonical subset)\n",
    "\n",
    "def score_best_fipe_for_key(brand_norm: str, model_norm: str, version_norm: str, model_year: int, fipe_df: pd.DataFrame, threshold: float):\n",
    "    if not version_norm or not model_norm or pd.isna(model_year):\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    m_tokens = [re.escape(t) for t in model_norm.split() if t]\n",
    "    if not m_tokens:\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    token_pattern = \"(?=.*\" + \")(?=.*\".join(m_tokens) + \")\"\n",
    "    cand_mask = (\n",
    "        (fipe_df[\"_brand_norm\"] == brand_norm) &\n",
    "        (fipe_df[\"AnoModelo\"] == int(model_year)) &\n",
    "        fipe_df[\"_model_norm\"].str.contains(token_pattern, regex=True, na=False)\n",
    "    )\n",
    "    cand = fipe_df[cand_mask]\n",
    "    if cand.empty:\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    v_toks = tokset(version_norm)\n",
    "    v_engine = extract_engine(version_norm)\n",
    "    s_best=-1.0; m_best=c_best=None; y_best=None\n",
    "    for _, fr in cand.iterrows():\n",
    "        fipe_model_norm = fr[\"_model_norm\"]\n",
    "        c_toks = fr[\"_toks\"]\n",
    "        q_toks = v_toks\n",
    "        inter = len(q_toks & c_toks)\n",
    "        coverage = inter / len(q_toks) if q_toks else 0.0\n",
    "        precision = inter / len(c_toks) if c_toks else 0.0\n",
    "        if coverage < 0.05: continue\n",
    "        f1 = (2*precision*coverage/(precision+coverage)) if (precision+coverage) else 0.0\n",
    "        jacc = (len(q_toks & c_toks)/len(q_toks | c_toks)) if (q_toks or c_toks) else 0.0\n",
    "        base = SequenceMatcher(None, version_norm, fipe_model_norm).ratio()\n",
    "        score = 0.55*f1 + 0.20*jacc + 0.25*base\n",
    "        c_engine = fr[\"_engine\"]\n",
    "        if v_engine:\n",
    "            score += 0.05 if c_engine == v_engine else (-0.10 if c_engine is not None else 0.0)\n",
    "        if \"gp\" in v_toks and \"gp\" in c_toks: score += 0.03\n",
    "        score -= min(0.12, 0.02 * len(c_toks - q_toks))\n",
    "        if score > s_best:\n",
    "            s_best = score; m_best = fr[\"Modelo\"]; c_best = fr[\"CodigoFipe\"]; y_best = int(fr[\"AnoModelo\"]) if pd.notna(fr[\"AnoModelo\"]) else None\n",
    "    if s_best >= threshold and m_best and c_best and y_best is not None:\n",
    "        return (m_best, c_best, y_best, float(round(s_best,4)), \"matched\")\n",
    "    return (None, None, None, float(max(s_best,0.0)), \"unmatched\")\n",
    "\n",
    "print('Canonical normalization & matching helpers loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165d7db",
   "metadata": {},
   "source": [
    "## 5. Loading Raw Localiza CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localiza_csv is None:\n",
    "    raise SystemExit('Localiza CSV not found. Place parsed CSV in raw/localiza/.')\n",
    "loc_df = pd.read_csv(localiza_csv, sep=';')\n",
    "log.info('Localiza rows: %d columns: %d', len(loc_df), loc_df.shape[1])\n",
    "required_loc_cols = {'brand','model','version','model_year','price'}\n",
    "missing = required_loc_cols - set(c.lower() for c in loc_df.columns)\n",
    "if missing:\n",
    "    log.warning('Localiza missing columns (may be fine if naming differs): %s', missing)\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88beabe8",
   "metadata": {},
   "source": [
    "## 6. Loading Raw Movida CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if movida_csv is None:\n",
    "    log.warning('Movida CSV not found; continuing with Localiza only.')\n",
    "    mov_df = pd.DataFrame(columns=['brand','model','version','model_year','price'])\n",
    "else:\n",
    "    mov_df = pd.read_csv(movida_csv, sep=';')\n",
    "    log.info('Movida rows: %d columns: %d', len(mov_df), mov_df.shape[1])\n",
    "mov_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0ef3a",
   "metadata": {},
   "source": [
    "## 7. Data Harmonization (Column Mapping & Typing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(df: pd.DataFrame, vendor: str) -> pd.DataFrame:\n",
    "    rename_map = {c.lower(): c for c in df.columns}\n",
    "    def col(name):\n",
    "        return next((c for k,c in rename_map.items() if k == name), None)\n",
    "    for required in ['brand','model','version']:\n",
    "        if col(required) is None:\n",
    "            df[required] = ''\n",
    "    if col('price') is None:\n",
    "        df['price'] = pd.NA\n",
    "    if col('model_year') is None:\n",
    "        df['model_year'] = pd.NA\n",
    "    if 'snapshot_date' not in df.columns:\n",
    "        df['snapshot_date'] = today_iso()\n",
    "    # Lowercase important fields\n",
    "    for c in ['brand','model','version']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "    df['price'] = df['price'].apply(clean_price_to_int).astype('Int64') if 'price' in df.columns else pd.NA\n",
    "    df['model_year'] = pd.to_numeric(df['model_year'], errors='coerce').astype('Int64')\n",
    "    return df\n",
    "\n",
    "loc_df = _standardize(loc_df, 'localiza')\n",
    "mov_df = _standardize(mov_df, 'movida')\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a0238",
   "metadata": {},
   "source": [
    "## 8. Augment Localiza & Movida (Derived Version Fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [loc_df, mov_df]:\n",
    "    if df.empty: continue\n",
    "    df['_brand_norm'] = df['brand'].map(norm_brand)\n",
    "    df['_model_norm'] = df['model'].map(generic_norm_text)\n",
    "    df['_version_norm'] = df['version'].map(norm_text)\n",
    "    df['_engine'] = df['_version_norm'].map(extract_engine)\n",
    "loc_df[['brand','model','version','_version_norm']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ae6eb",
   "metadata": {},
   "source": [
    "## 9. Load FIPE Models (fipe_models.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6230d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fipe_models_csv is None:\n",
    "    raise SystemExit('fipe_models.csv not found in data/. Download or copy it before proceeding.')\n",
    "# Flexible separator autodetect\n",
    "try:\n",
    "    fipe_models = pd.read_csv(fipe_models_csv, sep=None, engine='python', encoding='utf-8-sig')\n",
    "except Exception:\n",
    "    fipe_models = pd.read_csv(fipe_models_csv, sep=';', encoding='utf-8-sig')\n",
    "log.info('FIPE models rows: %d', len(fipe_models))\n",
    "fipe_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a22c89",
   "metadata": {},
   "source": [
    "## 10. Prepare FIPE Models (Normalization & Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f602ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fipe_models.columns = [c.replace('\\ufeff','').strip() for c in fipe_models.columns]\n",
    "colmap = {c.lower(): c for c in fipe_models.columns}\n",
    "# Required columns\n",
    "req_map = {k: colmap.get(k) for k in ['marca','modelo','codigofipe','anomodelo']}\n",
    "missing = [k for k,v in req_map.items() if v is None]\n",
    "if missing:\n",
    "    raise SystemExit(f'Missing required FIPE model columns: {missing}')\n",
    "fm = fipe_models.rename(columns={req_map['marca']:'Marca', req_map['modelo']:'Modelo', req_map['codigofipe']:'CodigoFipe', req_map['anomodelo']:'AnoModelo'})\n",
    "fm['Marca'] = fm['Marca'].astype(str).str.lower().str.strip()\n",
    "fm['Modelo'] = fm['Modelo'].astype(str).str.lower().str.strip()\n",
    "fm['CodigoFipe'] = fm['CodigoFipe'].astype(str).str.strip()\n",
    "fm['AnoModelo'] = pd.to_numeric(fm['AnoModelo'], errors='coerce').astype('Int64')\n",
    "fm = fm[fm['CodigoFipe'].ne('') & fm['Modelo'].ne('') & fm['AnoModelo'].notna()].copy()\n",
    "fm['_brand_norm'] = fm['Marca'].map(norm_brand)\n",
    "fm['_model_norm'] = fm['Modelo'].map(norm_text)\n",
    "fm['_toks'] = fm['_model_norm'].str.split().apply(set)\n",
    "fm['_engine'] = fm['_model_norm'].map(extract_engine)\n",
    "fm = fm.drop_duplicates(subset=['CodigoFipe','Modelo','AnoModelo']).reset_index(drop=True)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61112271",
   "metadata": {},
   "source": [
    "## 11. Load / Initialize Version Match Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f24ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_MATCH_TABLE = DATA_DIR / 'localiza_version_match.csv'\n",
    "match_cols = ['brand_norm','model_norm','version_norm','model_year','fipe_brand','fipe_model','fipe_code','score','match_source','first_seen','last_seen']\n",
    "if VERSION_MATCH_TABLE.exists():\n",
    "    cache_df = pd.read_csv(VERSION_MATCH_TABLE, sep=';')\n",
    "    for c in match_cols:\n",
    "        if c not in cache_df.columns: cache_df[c] = pd.NA\n",
    "    cache_df = cache_df[match_cols]\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=match_cols)\n",
    "cache_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8010f4c",
   "metadata": {},
   "source": [
    "## 12. Match Localiza Versions to FIPE Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.62  # adjust as needed\n",
    "\n",
    "# Identify new keys\n",
    "loc_df['model_year'] = pd.to_numeric(loc_df['model_year'], errors='coerce').astype('Int64')\n",
    "key_cols = ['_brand_norm','_model_norm','_version_norm','model_year']\n",
    "existing_keys = set(tuple(r) for r in cache_df[['brand_norm','model_norm','version_norm','model_year']].itertuples(index=False, name=None))\n",
    "new_keys_df = (loc_df[key_cols].drop_duplicates()\n",
    "               .rename(columns={'_brand_norm':'brand_norm','_model_norm':'model_norm','_version_norm':'version_norm'}))\n",
    "new_keys_df['model_year'] = pd.to_numeric(new_keys_df['model_year'], errors='coerce').astype('Int64')\n",
    "new_keys_df = new_keys_df[~new_keys_df.apply(tuple, axis=1).isin(existing_keys)]\n",
    "log.info('New version keys to match: %d', len(new_keys_df))\n",
    "\n",
    "rows = []\n",
    "today_iso = today_iso()\n",
    "for _, r in new_keys_df.iterrows():\n",
    "    brand_n = r['brand_norm']; model_n = r['model_norm']; version_n = r['version_norm']; year = r['model_year']\n",
    "    if pd.isna(year) or not model_n:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    cand = fm[(fm['_brand_norm']==brand_n) & (fm['AnoModelo']==int(year))]\n",
    "    if cand.empty:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    model_tok_set = set(model_n.split())\n",
    "    cand = cand[cand['_model_norm'].apply(lambda m: model_n in m or model_tok_set.issubset(set(m.split())))]\n",
    "    if cand.empty:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    v_toks = set(version_n.split()) if version_n else set()\n",
    "    best_score=-1.0; best=None\n",
    "    for _, fr in cand.iterrows():\n",
    "        f_toks = fr['_toks']\n",
    "        inter = len(v_toks & f_toks)\n",
    "        coverage = inter/len(v_toks) if v_toks else 0.0\n",
    "        precision = inter/len(f_toks) if f_toks else 0.0\n",
    "        f1 = (2*precision*coverage/(precision+coverage)) if (precision+coverage) else 0.0\n",
    "        jacc = (len(v_toks & f_toks)/len(v_toks | f_toks)) if (v_toks or f_toks) else 0.0\n",
    "        seq = SequenceMatcher(None, version_n, fr['_model_norm']).ratio() if version_n else 0.0\n",
    "        score = 0.5*seq + 0.3*f1 + 0.2*jacc\n",
    "        if score > best_score:\n",
    "            best_score = score; best = fr\n",
    "    if best is not None:\n",
    "        rows.append({**r, 'fipe_brand': best['Marca'], 'fipe_model': best['Modelo'], 'fipe_code': best['CodigoFipe'], 'score': round(float(best_score),4), 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "\n",
    "new_matches = pd.DataFrame(rows, columns=match_cols)\n",
    "new_matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfeadfd",
   "metadata": {},
   "source": [
    "## 13. Persist Updated Match Cache & Matched Localiza Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not new_matches.empty:\n",
    "    cache_df = pd.concat([cache_df, new_matches], ignore_index=True)\n",
    "# Update last_seen for keys present today\n",
    "present_keys = set(tuple(r) for r in loc_df[key_cols].drop_duplicates().rename(columns={'_brand_norm':'brand_norm','_model_norm':'model_norm','_version_norm':'version_norm'}).itertuples(index=False, name=None))\n",
    "mask = cache_df[['brand_norm','model_norm','version_norm','model_year']].apply(tuple, axis=1).isin(present_keys)\n",
    "cache_df.loc[mask,'last_seen'] = today_iso\n",
    "cache_df = cache_df.sort_values(['brand_norm','model_norm','version_norm','model_year','last_seen']).drop_duplicates(subset=['brand_norm','model_norm','version_norm','model_year'], keep='last')\n",
    "ensure_dir(VERSION_MATCH_TABLE.parent)\n",
    "cache_df.to_csv(VERSION_MATCH_TABLE, index=False, sep=';')\n",
    "log.info('Saved version match cache: %d rows', len(cache_df))\n",
    "\n",
    "# Merge back into Localiza dataset\n",
    "loc_matched = loc_df.merge(cache_df.rename(columns={'brand_norm':'_brand_norm','model_norm':'_model_norm','version_norm':'_version_norm'}), on=['_brand_norm','_model_norm','_version_norm','model_year'], how='left')\n",
    "loc_matched.rename(columns={'score':'match_score'}, inplace=True)\n",
    "loc_matched['match_score'] = pd.to_numeric(loc_matched['match_score'], errors='coerce').fillna(0.0)\n",
    "loc_matched['match_accepted'] = (loc_matched['match_score'] >= THRESHOLD).astype(int)\n",
    "match_out = DATA_DIR / f\"localiza_with_fipe_match_{ymd_compact()}.csv\"\n",
    "loc_matched.to_csv(match_out, index=False, sep=';')\n",
    "log.info('Saved matched Localiza CSV: %s', match_out)\n",
    "loc_matched[['brand','model','version','fipe_code','match_score','match_accepted']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae959f",
   "metadata": {},
   "source": [
    "## 14. Extract (fipe_code, model_year) Tuples from Localiza & Movida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuples(loc: pd.DataFrame, mov: pd.DataFrame):\n",
    "    out = set()\n",
    "    if not loc.empty and 'fipe_code' in loc.columns:\n",
    "        for _, r in loc[['fipe_code','model_year']].dropna().iterrows():\n",
    "            code = str(r['fipe_code']).strip();\n",
    "            try: yr = int(r['model_year'])\n",
    "            except: continue\n",
    "            if code:\n",
    "                out.add((code, yr))\n",
    "    if not mov.empty and 'fipe_code' in mov.columns:\n",
    "        for _, r in mov[['fipe_code','model_year']].dropna().iterrows():\n",
    "            code = str(r['fipe_code']).strip();\n",
    "            try: yr = int(r['model_year'])\n",
    "            except: continue\n",
    "            if code:\n",
    "                out.add((code, yr))\n",
    "    return out\n",
    "\n",
    "tuples_set = collect_tuples(loc_matched, mov_df)\n",
    "log.info('Unique tuples collected: %d', len(tuples_set))\n",
    "list(sorted(list(tuples_set))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf13170",
   "metadata": {},
   "source": [
    "## 15. Generate Tuples Audit DataFrame & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2583647",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = loc_matched[['fipe_code','model_year']].dropna().copy(); ldf['fipe_code']=ldf['fipe_code'].astype(str).str.strip()\n",
    "mdf = mov_df[['fipe_code','model_year']].dropna().copy() if ('fipe_code' in mov_df.columns) else pd.DataFrame(columns=['fipe_code','model_year'])\n",
    "if not mdf.empty:\n",
    "    mdf['fipe_code']=mdf['fipe_code'].astype(str).str.strip()\n",
    "\n",
    "lcnt = (ldf.groupby(['fipe_code','model_year']).size().rename('localiza_count').reset_index()) if not ldf.empty else pd.DataFrame(columns=['fipe_code','model_year','localiza_count'])\n",
    "cnt2 = (mdf.groupby(['fipe_code','model_year']).size().rename('movida_count').reset_index()) if not mdf.empty else pd.DataFrame(columns=['fipe_code','model_year','movida_count'])\n",
    "audit = pd.merge(lcnt, cnt2, on=['fipe_code','model_year'], how='outer')\n",
    "for c in ['localiza_count','movida_count']:\n",
    "    if c not in audit.columns: audit[c]=0\n",
    "audit[['localiza_count','movida_count']] = audit[['localiza_count','movida_count']].fillna(0).astype(int)\n",
    "audit['total_count'] = audit['localiza_count'] + audit['movida_count']\n",
    "audit = audit.sort_values(['total_count','fipe_code','model_year'], ascending=[False, True, True])\n",
    "TUPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "audit_out = TUPLES_DIR / f'fipe_tuples_{ymd_compact()}.csv'\n",
    "audit.to_csv(audit_out, index=False, sep=';')\n",
    "log.info('Tuples audit saved: %s (rows=%d)', audit_out, len(audit))\n",
    "audit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b7f61",
   "metadata": {},
   "source": [
    "## 16. Load Existing FIPE Dump (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285df96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fipe_dump_csv is None:\n",
    "    log.warning('No FIPE dump CSV found (data/fipe/fipe_dump_*.csv). Skip pricing merge steps if absent.')\n",
    "    fipe_dump = pd.DataFrame()\n",
    "else:\n",
    "    fipe_dump = pd.read_csv(fipe_dump_csv)\n",
    "    # Basic numeric parsing for ValorNum if present\n",
    "    if 'ValorNum' in fipe_dump.columns:\n",
    "        fipe_dump['ValorNum'] = pd.to_numeric(fipe_dump['ValorNum'], errors='coerce')\n",
    "log.info('FIPE dump rows: %d', len(fipe_dump))\n",
    "fipe_dump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b6752",
   "metadata": {},
   "source": [
    "## 17. Build Localiza Vendor Table (Merge FIPE Pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabef2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_vendor(df: pd.DataFrame, fipe: pd.DataFrame) -> pd.DataFrame:\n",
    "    if fipe.empty: return df.assign(fipe_price=pd.NA, premium_vs_fipe_price=pd.NA)\n",
    "    f = fipe.rename(columns={'CodigoFipe':'fipe_code','AnoModelo':'model_year','ValorNum':'fipe_price','Modelo':'fipe_version'}) if 'CodigoFipe' in fipe.columns else fipe.copy()\n",
    "    keep_cols = [c for c in ['fipe_code','model_year','fipe_price','fipe_version'] if c in f.columns]\n",
    "    f = f[keep_cols].dropna(subset=['fipe_code','model_year']) if {'fipe_code','model_year'}.issubset(f.columns) else f\n",
    "    merged = df.merge(f, on=['fipe_code','model_year'], how='left') if {'fipe_code','model_year'}.issubset(df.columns) else df.copy()\n",
    "    if 'price' in merged.columns and 'fipe_price' in merged.columns:\n",
    "        merged['premium_vs_fipe_price'] = np.where(merged['fipe_price'].gt(0), (merged['price']-merged['fipe_price'])/merged['fipe_price'], pd.NA)\n",
    "    return merged\n",
    "\n",
    "loc_vendor = _merge_vendor(loc_matched, fipe_dump)\n",
    "loc_vendor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f9037",
   "metadata": {},
   "source": [
    "## 18. Build Movida Vendor Table (Merge FIPE Pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fa947",
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_vendor = _merge_vendor(mov_df, fipe_dump)\n",
    "mov_vendor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088368f0",
   "metadata": {},
   "source": [
    "## 19. Build Consolidated FIPE Time Series Table (Filter to Vendor Presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mes_label(label: str):\n",
    "    pt = {'janeiro':1,'fevereiro':2,'março':3,'marco':3,'abril':4,'maio':5,'junho':6,'julho':7,'agosto':8,'setembro':9,'outubro':10,'novembro':11,'dezembro':12}\n",
    "    s = (label or '').lower().replace('\\xa0',' ').strip()\n",
    "    s = re.sub(r'\\s+',' ', s)\n",
    "    s_norm = re.sub(r'[/-]',' ', s)\n",
    "    m = re.search(r'(janeiro|fevereiro|março|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\s+(?:de\\s+)?(\\d{4})', s_norm)\n",
    "    if m:\n",
    "        return int(m.group(2)), pt[m.group(1)]\n",
    "    m2 = re.search(r'(1[0-2]|0?[1-9])\\s+(\\d{4})', s_norm)\n",
    "    if m2:\n",
    "        return int(m2.group(2)), int(m2.group(1))\n",
    "    return 1900,1\n",
    "\n",
    "if fipe_dump.empty:\n",
    "    fipe_series = pd.DataFrame()\n",
    "else:\n",
    "    fdf = fipe_dump.copy()\n",
    "    if 'MesReferencia' not in fdf.columns:\n",
    "        log.warning('Missing MesReferencia in FIPE dump; cannot build time series')\n",
    "        fipe_series = pd.DataFrame()\n",
    "    else:\n",
    "        fdf['reference_year'], fdf['reference_month'] = zip(*fdf['MesReferencia'].map(parse_mes_label))\n",
    "        # Shift back one month\n",
    "        def _shift(y,m):\n",
    "            return (y-1,12) if m==1 else (y, m-1)\n",
    "        fdf['reference_year'], fdf['reference_month'] = zip(*fdf.apply(lambda r: _shift(int(r['reference_year']), int(r['reference_month'])), axis=1))\n",
    "        fdf = fdf.rename(columns={'CodigoFipe':'fipe_code','AnoModelo':'model_year','ValorNum':'fipe_price','Marca':'brand','Modelo':'fipe_version'})\n",
    "        vendor_presence = set((c,y) for c,y in tuples_set)\n",
    "        fdf = fdf[fdf[['fipe_code','model_year']].apply(tuple, axis=1).isin(vendor_presence)].copy()\n",
    "        fdf = fdf.sort_values(['fipe_code','model_year','reference_year','reference_month'])\n",
    "        fdf = fdf.drop_duplicates(subset=['fipe_code','model_year','reference_year','reference_month'], keep='last')\n",
    "        fdf['m_m_price_change'] = fdf.groupby(['fipe_code','model_year'])['fipe_price'].pct_change()\n",
    "        fipe_series = fdf[['reference_year','reference_month','brand','fipe_version','fipe_code','model_year','fipe_price','m_m_price_change']]\n",
    "fipe_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8397c",
   "metadata": {},
   "source": [
    "## 20. Compute Price Premium & Month-over-Month Changes\n",
    "Already computed: premium_vs_fipe_price in vendor tables; m_m_price_change in FIPE series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56c29",
   "metadata": {},
   "source": [
    "## 21. Finalize & Export All Output Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55598fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = ymd_compact()\n",
    "ensure_dir(TABLES_DIR)\n",
    "loc_vendor_out = TABLES_DIR / f'localiza_table_{STAMP}.csv'\n",
    "mov_vendor_out = TABLES_DIR / f'movida_table_{STAMP}.csv'\n",
    "fipe_series_out = TABLES_DIR / f'fipe_table_{STAMP}.csv'\n",
    "loc_vendor.to_csv(loc_vendor_out, index=False, sep=';')\n",
    "mov_vendor.to_csv(mov_vendor_out, index=False, sep=';')\n",
    "if not fipe_series.empty:\n",
    "    fipe_series.to_csv(fipe_series_out, index=False, sep=';')\n",
    "log.info('Exported vendor tables and FIPE series.')\n",
    "\n",
    "loc_vendor.head(), mov_vendor.head(), fipe_series.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
