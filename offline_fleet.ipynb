{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe21c13",
   "metadata": {},
   "source": [
    "# Seminovos Fleet Processing Notebook\n",
    "\n",
    "This notebook lets you transform raw CSV files from Localiza & Movida plus FIPE reference data into clean, analysis‑ready tables. It reads files already saved under the `raw/` and `data/` folders.\n",
    "\n",
    "You can run each section (Shift+Enter) from top to bottom or Run All. If you update input CSVs later, you only need to re‑run from the “Path Configuration” section downward.\n",
    "\n",
    "---\n",
    "**High‑Level Output Files (all saved under `data/`):**\n",
    "- `data/localiza/localiza_with_fipe_match_YYYYMMDD.csv` – Localiza rows + matched FIPE code + score\n",
    "- `data/tuples/fipe_tuples_YYYYMMDD.csv` – Unique (FIPE code, model year) combinations observed\n",
    "- `data/tables/localiza_table_YYYYMMDD.csv` – Localiza vendor table enriched with FIPE price\n",
    "- `data/tables/movida_table_YYYYMMDD.csv` – Movida vendor table enriched with FIPE price\n",
    "- `data/tables/fipe_table_YYYYMMDD.csv` – FIPE monthly price time series\n",
    "\n",
    "If something fails, read the Troubleshooting section near the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda50b9",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "- Normalization: Cleaning text so similar things look the same (e.g. accents removed, abbreviations expanded).\n",
    "- Token: A single word after cleaning. We compare sets of tokens to measure similarity.\n",
    "- FIPE code: Official identifier for a vehicle model/year in the FIPE price table.\n",
    "- Match score: Number from 0 to ~1 estimating how good the FIPE match is (higher is better).\n",
    "- Threshold: Minimum score we trust automatically.\n",
    "- Premium vs FIPE: (Vendor asking price – FIPE reference price) / FIPE reference price.\n",
    "- Tuple: A pair (FIPE code, model year) representing a unique market unit.\n",
    "- Cache: Saved historical matches so we avoid recomputing the same work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdaae30",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "1. Place new raw CSVs under `raw/localiza/` and `raw/movida/` (naming pattern already used in folder).\n",
    "2. Ensure `data/fipe_models.csv` exists (mandatory) and a FIPE dump in `data/fipe/`.\n",
    "3. Run each cell from top to bottom (Run All works too).\n",
    "4. Grab exported tables from `data/tables/`.\n",
    "\n",
    "If you only updated one vendor file, you still run everything – it will be fast because matching cache skips previously seen versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Imports\n",
    "# (You just run this cell – it prepares libraries and logging.)\n",
    "import pandas as pd, numpy as np, re, json, logging, math\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime\n",
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "\n",
    "# Display tweaks: wider tables in notebook\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "# Simple logger so later steps show progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)7s | %(message)s')\n",
    "log = logging.getLogger('offline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14aa1e3",
   "metadata": {},
   "source": [
    "## 2. Path Configuration & Input File Selection\n",
    "Detect where the input CSVs live and pick the most recent ones automatically.\n",
    "\n",
    "Folders used (created if missing):\n",
    "- `raw/localiza/` – historical Localiza CSV exports (naming: `localiza_seminovos_YYYYMMDD.csv`)\n",
    "- `raw/movida/` – historical Movida CSV exports (naming: `movida_seminovos_YYYYMMDD.csv`)\n",
    "- `data/` – working data (e.g. `fipe_models.csv`)\n",
    "- `data/fipe/` – FIPE price dump snapshots (`fipe_dump_YYYYMMDD.csv`)\n",
    "\n",
    "What this section does:\n",
    "1. Creates folders if they do not exist.\n",
    "2. Picks the *latest* file matching each pattern based on modification time.\n",
    "3. Logs which files will be used.\n",
    "\n",
    "If a file you want is NOT selected (e.g. you added a newer one but want an older snapshot), manually set the variables after the cell runs, e.g.:\n",
    "```\n",
    "localiza_csv = Path('raw/localiza/localiza_seminovos_20250824.csv')\n",
    "```\n",
    "Then re-run from the next section onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e84821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect helper\n",
    "# Picks the most recently modified file matching \"pattern\" inside a folder.\n",
    "def latest(pattern: str, base: Path) -> Path | None:\n",
    "    files = sorted(base.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "# Define core directories (relative to this notebook location)\n",
    "BASE = Path.cwd().parent / 'Seminovos'\n",
    "DATA_DIR = BASE / 'data'\n",
    "RAW_DIR = BASE / 'raw'\n",
    "RAW_LOCALIZA = RAW_DIR / 'localiza'\n",
    "RAW_MOVIDA = RAW_DIR / 'movida'\n",
    "FIPE_DIR = DATA_DIR / 'fipe'\n",
    "TUPLES_DIR = DATA_DIR / 'tuples'\n",
    "TABLES_DIR = DATA_DIR / 'tables'\n",
    "VERSION_MATCH_TABLE = DATA_DIR / 'localiza_version_match.csv'\n",
    "MATCH_DIR = DATA_DIR / 'localiza'\n",
    "\n",
    "for d in [DATA_DIR, RAW_LOCALIZA, RAW_MOVIDA, FIPE_DIR, TUPLES_DIR, TABLES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Auto-pick newest snapshots\n",
    "# If you need to override manually for picking an older snapshot, set e.g.:\n",
    "# localiza_csv = Path('raw/localiza/localiza_seminovos_20250824.csv')\n",
    "localiza_csv = latest('localiza_seminovos_*.csv', RAW_LOCALIZA)\n",
    "movida_csv = latest('movida_seminovos_*.csv', RAW_MOVIDA)\n",
    "# FIPE models master (must exist)\n",
    "fipe_models_csv = (DATA_DIR / 'fipe_models.csv') if (DATA_DIR / 'fipe_models.csv').exists() else Path('fipe_models.csv')\n",
    "if not fipe_models_csv.exists(): fipe_models_csv = None\n",
    "# FIPE dump with pricing\n",
    "fipe_dump_csv = latest('fipe_dump_*.csv', FIPE_DIR)\n",
    "\n",
    "log.info('Localiza CSV: %s', localiza_csv)\n",
    "log.info('Movida   CSV: %s', movida_csv)\n",
    "log.info('FIPE models CSV: %s', fipe_models_csv)\n",
    "log.info('FIPE dump CSV: %s', fipe_dump_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c204b4",
   "metadata": {},
   "source": [
    "## 3. Utility Functions (Dates, Logging, Helpers)\n",
    "Small helper functions used later:\n",
    "- Date stamps for file names (so outputs never overwrite previous days)\n",
    "- Safe directory creation\n",
    "- Price cleaning: turns messy price strings (\"R$ 58.900,00\") into an integer number of Reais (58900)\n",
    "\n",
    "You normally do **not** need to edit anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility helpers ---\n",
    "\n",
    "def ymd_compact(d: date | None = None) -> str:\n",
    "    \"\"\"Return YYYYMMDD string for filenames (defaults to today).\"\"\"\n",
    "    return (d or date.today()).strftime('%Y%m%d')\n",
    "\n",
    "def today_iso() -> str:\n",
    "    \"\"\"Return ISO date YYYY-MM-DD for logging / columns.\"\"\"\n",
    "    return date.today().isoformat()\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    \"\"\"Create directory (and parents) if absent (no error if it exists).\"\"\"\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clean_price_to_int(x):\n",
    "    \"\"\"Normalize a price value (string like 'R$ 58.900,00' or number) to an int or <NA>.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)):\n",
    "        return pd.NA\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        return int(round(float(x)))\n",
    "    # Remove currency symbols & thousand separators, unify decimal marker\n",
    "    s = re.sub(r'[^\\d\\.,]', '', str(x))\n",
    "    if ',' in s and '.' in s:\n",
    "        s = s.replace('.', '').replace(',', '.')  # \"58.900,00\" -> \"58900.00\"\n",
    "    elif ',' in s:\n",
    "        s = s.replace(',', '.')\n",
    "    try:\n",
    "        return int(round(float(s)))\n",
    "    except Exception:\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7222b",
   "metadata": {},
   "source": [
    "## 4. Normalization Helpers (Accents, Text Cleaning, Tokens)\n",
    "Why we normalize:\n",
    "Marketplace data has many variations (accents, punctuation, abbreviations: e.g. \"Hig\" vs \"Highline\"). To reliably match vehicles to FIPE we convert text into a *canonical* form.\n",
    "\n",
    "Key concepts:\n",
    "- strip_accents → \"caminhão\" becomes \"caminhao\"\n",
    "- norm_text → heavy cleaning + standardizing abbreviations (\"aut\", \"mec\", model trims, etc.)\n",
    "- generic_norm_text → lighter cleaning when we only need broad grouping\n",
    "- norm_brand → groups aliases to a single brand (\"gm\", \"chevrolet\")\n",
    "- tokset → converts normalized string into a set of unique words (tokens) for similarity scoring\n",
    "- score_best_fipe_for_key → computes the best FIPE model candidate with a relevance score\n",
    "\n",
    "Unless you add new brand aliases or trim patterns, leave this section unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e19ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical normalization & helper functions (from fleet.py)\n",
    "# These functions standardize messy vendor text so we can compare with FIPE data.\n",
    "import re, unicodedata, pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Text Normalization Core ----------------\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", str(s))\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def remove_duplicate_words(text: str) -> str:\n",
    "    # Keep first occurrence of each word (case-insensitive) to reduce noise\n",
    "    seen = set(); result=[]\n",
    "    for word in text.split():\n",
    "        key = word.casefold()\n",
    "        if key not in seen:\n",
    "            seen.add(key); result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    \"\"\"Aggressive normalization of version names (trims, engines, special editions).\"\"\"\n",
    "    import pandas as pd, re\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(s).lower())\n",
    "    # Standardize punctuation / separators\n",
    "    s0 = s0.replace(\",\", \".\")\n",
    "    s0 = s0.replace(\"c/ar\", \"\").replace(\"c/ ar\",\"\")\n",
    "    s0 = re.sub(r'/', ' ', s0)\n",
    "    # Replace abbreviations & partial forms\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:T\\.)(?!\\w)', 'turbo', s0)\n",
    "    s0 = re.sub(r\"[^a-z0-9\\.\\s]\", \" \", s0)\n",
    "    s0 = re.sub(r\"\\bautomatic[oa]\\b|\\bat\\b|\\baut(?:\\.|o)?\\b\", \"aut\", s0)\n",
    "    s0 = re.sub(r\"\\bman(?:ual)?\\b|\\bmecanico\\b\", \"mec\", s0)\n",
    "    s0 = re.sub(r\"\\bt\\s?si\\b\", \"tsi\", s0)\n",
    "    s0 = re.sub(r\"\\b(\\d{2,4}(?:i|d))\\s*a\\b\", r\"\\1 aut\", s0)\n",
    "    # Remove weird dots inside words\n",
    "    s0 = re.sub(r'(?<=[A-Za-z])\\.(?=[A-Za-z])', '. ', s0)\n",
    "    s0 = re.sub(r'(?<=[A-Za-z])\\.', '', s0)\n",
    "    # Normalize common marketing trim abbreviations\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:perf|perfor|performa|performance|p)(?!\\w)', 'performance', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:long)(?!\\w)', 'longitude', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:sportb|SPB|SB)(?!\\w)', 'sportback', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:prest)(?!\\w)', 'prestige', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:ultim)(?!\\w)', 'ultimate', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:insc)(?!\\w)', 'inscription', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:xdrive30e)(?!\\w)', 'xdrive 30e', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:cp)(?!\\w)', 'cs plus', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:7l)(?!\\w)', '', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:hurric|hurr)(?!\\w)', 'hurricane', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:overl)(?!\\w)', 'overland', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:dies|die)(?!\\w)', 'diesel', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:tb)(?!\\w)', 'turbo', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:sed)(?!\\w)', 'sedan', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:step)(?!\\w)', 'stepway', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:hig)(?!\\w)', 'highline', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:limit)(?!\\w)', 'limited', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:plat)(?!\\w)', 'platinum', s0)\n",
    "    s0 = re.sub(r'(?i)\\b\\d+[pv]\\b', '', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:exclu)(?!\\w)', 'exclusive', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:t270)(?!\\w)', 'turbo 270', s0)\n",
    "    s0 = re.sub(r'(?i)(?<!\\w)(?:comfort|comfor)(?!\\w)', 'comfortline', s0)\n",
    "    # Specific mapping corrections\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+HATCH\\s+PREM\\.\\b', 'onix hatch premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SEDAN\\s+PREM\\.\\b', 'onix sedan premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SEDAN\\s+Plus+\\s+PREM\\.\\b', 'onix sedan plus premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bONIX\\s+SD\\.\\s+P\\.\\s+PR\\.\\b', 'onix sedan plus premier', s0)\n",
    "    s0 = re.sub(r'(?i)\\bFastback\\s+Limited+\\s+Ed\\.\\b', 'fastback limited edition', s0)\n",
    "    s0 = re.sub(r'(?i)\\bAIRCROSS\\s+F\\.\\b', 'aircross feel', s0)\n",
    "    # Put space in xc60, xc90 etc. if needed for uniform tokenization\n",
    "    s0 = re.sub(r'(?<=xc)(\\d+)', r' \\1', s0)\n",
    "    # Remove stray marketing \"new\" unless part of specific model names\n",
    "    s0 = re.sub(r'\\bnew\\b(?![\\s-]*(?:range|beetle)\\b)', '', s0, flags=re.IGNORECASE)\n",
    "    s0 = remove_duplicate_words(s0)\n",
    "    s0 = re.sub(r\"\\s+\", \" \", s0).strip()\n",
    "    return s0\n",
    "\n",
    "def generic_norm_text(s: str) -> str:\n",
    "    \"\"\"Lighter normalization: keep only alphanumerics + collapse spaces.\"\"\"\n",
    "    import re\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(s).lower())\n",
    "    s0 = re.sub(r\"[^a-z0-9\\s]\", \" \", s0)\n",
    "    s0 = re.sub(r'(?<=xc)(\\d+)', r' \\1', s0)\n",
    "    s0 = re.sub(r\"\\s+\", \" \", s0).strip()\n",
    "    return s0\n",
    "\n",
    "def norm_brand(s: str) -> str:\n",
    "    \"\"\"Brand alias consolidation (add here if new alias surfaces).\"\"\"\n",
    "    s0 = generic_norm_text(s)\n",
    "    aliases = {\n",
    "        \"vw - volkswagen\":\"volkswagen\",\"vw volkswagen\":\"volkswagen\",\"volks\":\"volkswagen\",\"volkswagem\":\"volkswagen\",\n",
    "        \"gm\":\"chevrolet\",\"gm - chevrolet\":\"chevrolet\",\"gm chevrolet\":\"chevrolet\",\"chevy\":\"chevrolet\",\n",
    "        \"mercedes-benz\":\"mercedes benz\",\"mb\":\"mercedes benz\",\"caoa chery\":\"chery\",\"caoa chery/chery\":\"chery\",\"great wall\":\"gwm\"\n",
    "    }\n",
    "    return aliases.get(s0, s0)\n",
    "\n",
    "def tokset(s: str) -> set:\n",
    "    return set(generic_norm_text(s).split())\n",
    "\n",
    "def extract_engine(s: str):\n",
    "    m = re.search(r\"\\b(\\d\\.\\d)\\b\", str(s))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# ---------------------------- Type normalization  -----------------------------\n",
    "\n",
    "def normalize_type(raw_type, fuel_sigla):\n",
    "    if fuel_sigla:\n",
    "        fs = str(fuel_sigla).upper().strip()\n",
    "        if fs in {'E','H'}:\n",
    "            return 'ev'\n",
    "    if raw_type is None or (isinstance(raw_type, float) and pd.isna(raw_type)):\n",
    "        return \"\"\n",
    "    s0 = strip_accents(str(raw_type)).upper().strip()\n",
    "    mapping = {\n",
    "        'SEDAN':'SEDAN','SEDÃ':'SEDAN','SEDA':'SEDAN',\n",
    "        'HATCH':'HATCH','HATCHBACK':'HATCH',\n",
    "        'SUV':'SUV',\n",
    "        'PICAPE':'PICKUP/VANS','PICAPE CABINE DUPLA':'PICKUP/VANS','PICK-UP':'PICKUP/VANS','PICKUP':'PICKUP/VANS','CAMINHONETE':'PICKUP/VANS','CABINE SIMPLES':'PICKUP/VANS',\n",
    "        'UTILITÁRIO':'PICKUP/VANS', 'UTILITARIO':'PICKUP/VANS','FURGAO':'PICKUP/VANS','MINIVAN':'PICKUP/VANS','VAN':'PICKUP/VANS','CARGA':'PICKUP/VANS',\n",
    "        'ELETRICO':'EV','HIBRIDO':'EV',\n",
    "        'COUPE':'PREMIUM','SPORTBACK':'PREMIUM','GRAN COUPE':'PREMIUM','FASTBACK':'PREMIUM',\n",
    "        'PARTICULAR':'OTHER','OUTROS':'OTHER','OUTRO':'OTHER'\n",
    "    }\n",
    "    return mapping.get(s0, s0).lower()\n",
    "\n",
    "# ---------------- Matching scorer (simplified canonical subset) ----------------\n",
    "\n",
    "def score_best_fipe_for_key(brand_norm: str, model_norm: str, version_norm: str, model_year: int, fipe_df: pd.DataFrame, threshold: float):\n",
    "    \"\"\"Return best (Model, FIPE code, Year, Score, status) for provided normalized key.\"\"\"\n",
    "    if not version_norm or not model_norm or pd.isna(model_year):\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    m_tokens = [re.escape(t) for t in model_norm.split() if t]\n",
    "    if not m_tokens:\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    token_pattern = \"(?=.*\" + \")(?=.*\".join(m_tokens) + \")\"\n",
    "    cand_mask = (\n",
    "        (fipe_df[\"_brand_norm\"] == brand_norm) &\n",
    "        (fipe_df[\"AnoModelo\"] == int(model_year)) &\n",
    "        fipe_df[\"_model_norm\"].str.contains(token_pattern, regex=True, na=False)\n",
    "    )\n",
    "    cand = fipe_df[cand_mask]\n",
    "    if cand.empty:\n",
    "        return (None, None, None, 0.0, \"unmatched\")\n",
    "    v_toks = tokset(version_norm)\n",
    "    v_engine = extract_engine(version_norm)\n",
    "    s_best=-1.0; m_best=c_best=None; y_best=None\n",
    "    for _, fr in cand.iterrows():\n",
    "        fipe_model_norm = fr[\"_model_norm\"]\n",
    "        c_toks = fr[\"_toks\"]\n",
    "        q_toks = v_toks\n",
    "        inter = len(q_toks & c_toks)\n",
    "        coverage = inter / len(q_toks) if q_toks else 0.0\n",
    "        precision = inter / len(c_toks) if c_toks else 0.0\n",
    "        if coverage < 0.05: continue\n",
    "        f1 = (2*precision*coverage/(precision+coverage)) if (precision+coverage) else 0.0\n",
    "        jacc = (len(q_toks & c_toks)/len(q_toks | c_toks)) if (q_toks or c_toks) else 0.0\n",
    "        base = SequenceMatcher(None, version_norm, fipe_model_norm).ratio()\n",
    "        score = 0.55*f1 + 0.20*jacc + 0.25*base\n",
    "        c_engine = fr[\"_engine\"]\n",
    "        if v_engine:\n",
    "            score += 0.05 if c_engine == v_engine else (-0.10 if c_engine is not None else 0.0)\n",
    "        if \"gp\" in v_toks and \"gp\" in c_toks: score += 0.03\n",
    "        score -= min(0.12, 0.02 * len(c_toks - q_toks))  # Small penalty for extra unmatched tokens\n",
    "        if score > s_best:\n",
    "            s_best = score; m_best = fr[\"Modelo\"]; c_best = fr[\"CodigoFipe\"]; y_best = int(fr[\"AnoModelo\"]) if pd.notna(fr[\"AnoModelo\"]) else None\n",
    "    if s_best >= threshold and m_best and c_best and y_best is not None:\n",
    "        return (m_best, c_best, y_best, float(round(s_best,4)), \"matched\")\n",
    "    return (None, None, None, float(max(s_best,0.0)), \"unmatched\")\n",
    "\n",
    "print('Canonical normalization & matching helpers loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165d7db",
   "metadata": {},
   "source": [
    "## 5. Loading Raw Localiza CSV\n",
    "Reads the latest Localiza snapshot selected earlier. Expected columns (case-insensitive):\n",
    "- brand, model, version, model_year, price (others are kept if present)\n",
    "\n",
    "If column names differ slightly the pipeline still tries to adapt. If the file is missing this cell stops execution with a clear message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if localiza_csv is None:\n",
    "    raise SystemExit('Localiza CSV not found. Place parsed CSV in raw/localiza/.')\n",
    "loc_df = pd.read_csv(localiza_csv, sep=';')\n",
    "log.info('Localiza rows: %d columns: %d', len(loc_df), loc_df.shape[1])\n",
    "required_loc_cols = {'brand','model','version','model_year','price'}\n",
    "missing = required_loc_cols - set(c.lower() for c in loc_df.columns)\n",
    "if missing:\n",
    "    log.warning('Localiza missing columns (may be fine if naming differs): %s', missing)\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88beabe8",
   "metadata": {},
   "source": [
    "## 6. Loading Raw Movida CSV\n",
    "Same logic as Localiza. If no Movida file is found we continue with an empty Movida DataFrame (the rest of the pipeline still works – Movida outputs will just be empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if movida_csv is None:\n",
    "    log.warning('Movida CSV not found; continuing with Localiza only.')\n",
    "    mov_df = pd.DataFrame(columns=['brand','model','version','model_year','price'])\n",
    "else:\n",
    "    mov_df = pd.read_csv(movida_csv, sep=';')\n",
    "    log.info('Movida rows: %d columns: %d', len(mov_df), mov_df.shape[1])\n",
    "mov_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0ef3a",
   "metadata": {},
   "source": [
    "## 7. Data Harmonization (Column Mapping & Typing)\n",
    "We standardize both vendors to a common schema and data types:\n",
    "- Force text fields to lowercase trimmed strings\n",
    "- Convert prices to integer Reais\n",
    "- Convert model_year to integer (dropping invalid years)\n",
    "- Add a snapshot_date (today) if not present\n",
    "\n",
    "Result: both DataFrames are comparable downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(df: pd.DataFrame, vendor: str) -> pd.DataFrame:\n",
    "    \"\"\"Ensure required columns exist and enforce consistent types.\"\"\"\n",
    "    rename_map = {c.lower(): c for c in df.columns}\n",
    "    def col(name):\n",
    "        return next((c for k,c in rename_map.items() if k == name), None)\n",
    "    # Create missing mandatory text columns\n",
    "    for required in ['brand','model','version']:\n",
    "        if col(required) is None:\n",
    "            df[required] = ''\n",
    "    # Create defaults if price/year absent\n",
    "    if col('price') is None:\n",
    "        df['price'] = pd.NA\n",
    "    if col('model_year') is None:\n",
    "        df['model_year'] = pd.NA\n",
    "    if 'snapshot_date' not in df.columns:\n",
    "        df['snapshot_date'] = today_iso()\n",
    "    # Standardize casing\n",
    "    for c in ['brand','model','version']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "    # Normalize price & year\n",
    "    df['price'] = df['price'].apply(clean_price_to_int).astype('Int64') if 'price' in df.columns else pd.NA\n",
    "    df['model_year'] = pd.to_numeric(df['model_year'], errors='coerce').astype('Int64')\n",
    "    return df\n",
    "\n",
    "loc_df = _standardize(loc_df, 'localiza')\n",
    "mov_df = _standardize(mov_df, 'movida')\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a0238",
   "metadata": {},
   "source": [
    "## 8. Augment Localiza (Derived Version Fields)\n",
    "Adds normalized columns used for matching:\n",
    "- _brand_norm (after aliasing)\n",
    "- _model_norm (generic normalization)\n",
    "- _version_norm (detailed normalization of the full version string)\n",
    "- _engine (engine size like 1.0, 1.3, 2.0 if detected)\n",
    "\n",
    "These are helper features for similarity scoring vs FIPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [loc_df]:\n",
    "    if df.empty: continue\n",
    "    df['_brand_norm'] = df['brand'].map(norm_brand)\n",
    "    df['_model_norm'] = df['model'].map(generic_norm_text)\n",
    "    df['_version_norm'] = df['version'].map(norm_text)\n",
    "    df['_engine'] = df['_version_norm'].map(extract_engine)\n",
    "loc_df[['brand','model','version','_version_norm']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ae6eb",
   "metadata": {},
   "source": [
    "## 9. Load FIPE Models (fipe_models.csv)\n",
    "Loads the master list of FIPE (Marca, Modelo, CodigoFipe, AnoModelo). This file must already be exported and placed under `data/`.\n",
    "\n",
    "If the file is missing we stop, because no matching can happen without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6230d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fipe_models_csv is None:\n",
    "    raise SystemExit('fipe_models.csv not found in data/. Download or copy it before proceeding.')\n",
    "# Flexible separator autodetect\n",
    "try:\n",
    "    fipe_models = pd.read_csv(fipe_models_csv, sep=None, engine='python', encoding='utf-8-sig')\n",
    "except Exception:\n",
    "    fipe_models = pd.read_csv(fipe_models_csv, sep=';', encoding='utf-8-sig')\n",
    "log.info('FIPE models rows: %d', len(fipe_models))\n",
    "fipe_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a22c89",
   "metadata": {},
   "source": [
    "## 10. Prepare FIPE Models (Normalization & Tokenization)\n",
    "We clean & normalize FIPE models so they are comparable with vendor versions:\n",
    "- Lowercase & strip spaces\n",
    "- Apply same normalization to brand/model\n",
    "- Tokenize each model into `_toks` (a set of unique words)\n",
    "- Extract engine displacement for extra scoring tie‑breakers\n",
    "- Remove duplicates (same code, model, year)\n",
    "\n",
    "Output: DataFrame `fm` used for version matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f602ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fipe_models.columns = [c.replace('\\ufeff','').strip() for c in fipe_models.columns]\n",
    "colmap = {c.lower(): c for c in fipe_models.columns}\n",
    "# Required columns\n",
    "req_map = {k: colmap.get(k) for k in ['marca','modelo','codigofipe','anomodelo']}\n",
    "missing = [k for k,v in req_map.items() if v is None]\n",
    "if missing:\n",
    "    raise SystemExit(f'Missing required FIPE model columns: {missing}')\n",
    "fm = fipe_models.rename(columns={req_map['marca']:'Marca', req_map['modelo']:'Modelo', req_map['codigofipe']:'CodigoFipe', req_map['anomodelo']:'AnoModelo'})\n",
    "fm['Marca'] = fm['Marca'].astype(str).str.lower().str.strip()\n",
    "fm['Modelo'] = fm['Modelo'].astype(str).str.lower().str.strip()\n",
    "fm['CodigoFipe'] = fm['CodigoFipe'].astype(str).str.strip()\n",
    "fm['AnoModelo'] = pd.to_numeric(fm['AnoModelo'], errors='coerce').astype('Int64')\n",
    "fm = fm[fm['CodigoFipe'].ne('') & fm['Modelo'].ne('') & fm['AnoModelo'].notna()].copy()\n",
    "fm['_brand_norm'] = fm['Marca'].map(norm_brand)\n",
    "fm['_model_norm'] = fm['Modelo'].map(norm_text)\n",
    "fm['_toks'] = fm['_model_norm'].str.split().apply(set)\n",
    "fm['_engine'] = fm['_model_norm'].map(extract_engine)\n",
    "fm = fm.drop_duplicates(subset=['CodigoFipe','Modelo','AnoModelo']).reset_index(drop=True)\n",
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61112271",
   "metadata": {},
   "source": [
    "## 11. Load / Initialize Version Match Cache\n",
    "We keep a historical CSV (`data/localiza_version_match.csv`) so that once a (brand_norm, model_norm, version_norm, model_year) is matched to a FIPE code we do **not** need to recompute it every time.\n",
    "\n",
    "Columns tracked:\n",
    "- fipe_code + fipe_model + score\n",
    "- match_source (method used)\n",
    "- first_seen / last_seen (aging & churn analysis)\n",
    "\n",
    "If the file does not exist we start with an empty cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f24ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = ['brand_norm','model_norm','version_norm','model_year','fipe_brand','fipe_model','fipe_code','score','match_source','first_seen','last_seen']\n",
    "# Load version match cache\n",
    "if VERSION_MATCH_TABLE.exists():\n",
    "    cache_df = pd.read_csv(VERSION_MATCH_TABLE, sep=';')\n",
    "    for c in match_cols:\n",
    "        if c not in cache_df.columns: cache_df[c] = pd.NA\n",
    "    cache_df = cache_df[match_cols]\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=match_cols)\n",
    "cache_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8010f4c",
   "metadata": {},
   "source": [
    "## 12. Match Localiza Versions to FIPE Codes\n",
    "For each new (brand_norm, model_norm, version_norm, model_year) key not already in the cache:\n",
    "1. Filter FIPE candidates to same brand and year.\n",
    "2. Keep candidates where FIPE model name contains the normalized model or all its tokens.\n",
    "3. Score each candidate using token overlap & text similarity.\n",
    "4. Pick the best candidate and record its FIPE code + score.\n",
    "\n",
    "The threshold controls which matches are considered good enough later.\n",
    "\n",
    "Result: `new_matches` DataFrame (may be empty if nothing new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.0  # Using 0.0 to avoid versions with no matches\n",
    "\n",
    "# Identify new keys (those not already in the cache)\n",
    "loc_df['model_year'] = pd.to_numeric(loc_df['model_year'], errors='coerce').astype('Int64')\n",
    "key_cols = ['_brand_norm','_model_norm','_version_norm','model_year']\n",
    "existing_keys = set(tuple(r) for r in cache_df[['brand_norm','model_norm','version_norm','model_year']].itertuples(index=False, name=None))\n",
    "new_keys_df = (loc_df[key_cols].drop_duplicates()\n",
    "               .rename(columns={'_brand_norm':'brand_norm','_model_norm':'model_norm','_version_norm':'version_norm'}))\n",
    "new_keys_df['model_year'] = pd.to_numeric(new_keys_df['model_year'], errors='coerce').astype('Int64')\n",
    "new_keys_df = new_keys_df[~new_keys_df.apply(tuple, axis=1).isin(existing_keys)]\n",
    "log.info('New version keys to match: %d', len(new_keys_df))\n",
    "\n",
    "rows = []\n",
    "today_iso = today_iso()\n",
    "for _, r in new_keys_df.iterrows():\n",
    "    brand_n = r['brand_norm']; model_n = r['model_norm']; version_n = r['version_norm']; year = r['model_year']\n",
    "    # Basic validation: need model tokens + year\n",
    "    if pd.isna(year) or not model_n:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    # Narrow FIPE candidates by brand & year\n",
    "    cand = fm[(fm['_brand_norm']==brand_n) & (fm['AnoModelo']==int(year))]\n",
    "    if cand.empty:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    model_tok_set = set(model_n.split())\n",
    "    # Require all model tokens to appear (or substring containment)\n",
    "    cand = cand[cand['_model_norm'].apply(lambda m: model_n in m or model_tok_set.issubset(set(m.split())))]\n",
    "    if cand.empty:\n",
    "        rows.append({**r, 'fipe_brand': None, 'fipe_model': None, 'fipe_code': None, 'score': 0.0, 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "        continue\n",
    "    # Score remaining candidates\n",
    "    v_toks = set(version_n.split()) if version_n else set()\n",
    "    best_score=-1.0; best=None\n",
    "    for _, fr in cand.iterrows():\n",
    "        f_toks = fr['_toks']\n",
    "        # Compute token intersection\n",
    "        inter = len(v_toks & f_toks)\n",
    "        # Compute coverage and precision\n",
    "        coverage = inter/len(v_toks) if v_toks else 0.0\n",
    "        precision = inter/len(f_toks) if f_toks else 0.0\n",
    "        # Compute F1 score\n",
    "        f1 = (2*precision*coverage/(precision+coverage)) if (precision+coverage) else 0.0\n",
    "        # Compute Jaccard similarity\n",
    "        jacc = (len(v_toks & f_toks)/len(v_toks | f_toks)) if (v_toks or f_toks) else 0.0\n",
    "        # Compute sequence similarity\n",
    "        seq = SequenceMatcher(None, version_n, fr['_model_norm']).ratio() if version_n else 0.0\n",
    "        # Compute overall score\n",
    "        score = 0.5*seq + 0.3*f1 + 0.2*jacc\n",
    "        if score > best_score:\n",
    "            best_score = score; best = fr\n",
    "    if best is not None:\n",
    "        rows.append({**r, 'fipe_brand': best['Marca'], 'fipe_model': best['Modelo'], 'fipe_code': best['CodigoFipe'], 'score': round(float(best_score),4), 'match_source':'contains','first_seen':today_iso,'last_seen':today_iso})\n",
    "\n",
    "new_matches = pd.DataFrame(rows, columns=match_cols)\n",
    "new_matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfeadfd",
   "metadata": {},
   "source": [
    "## 13. Persist Updated Match Cache & Matched Localiza Dataset\n",
    "Steps:\n",
    "1. Append any new matches to the cache.\n",
    "2. Refresh `last_seen` for keys present today.\n",
    "3. Save cache back to disk (so next time we reuse it).\n",
    "4. Merge matches into Localiza rows producing: `localiza_with_fipe_match_YYYYMMDD.csv`.\n",
    "5. Flag `match_accepted` when score >= threshold.\n",
    "\n",
    "You can inspect the head below to sanity check a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not new_matches.empty:\n",
    "    cache_df = pd.concat([cache_df, new_matches], ignore_index=True)\n",
    "# Update last_seen for keys present today\n",
    "present_keys = set(tuple(r) for r in loc_df[key_cols].drop_duplicates().rename(columns={'_brand_norm':'brand_norm','_model_norm':'model_norm','_version_norm':'version_norm'}).itertuples(index=False, name=None))\n",
    "mask = cache_df[['brand_norm','model_norm','version_norm','model_year']].apply(tuple, axis=1).isin(present_keys)\n",
    "cache_df.loc[mask,'last_seen'] = today_iso\n",
    "cache_df = cache_df.sort_values(['brand_norm','model_norm','version_norm','model_year','last_seen']).drop_duplicates(subset=['brand_norm','model_norm','version_norm','model_year'], keep='last')\n",
    "ensure_dir(VERSION_MATCH_TABLE.parent)\n",
    "cache_df.to_csv(VERSION_MATCH_TABLE, index=False, sep=';')\n",
    "log.info('Saved version match cache: %d rows', len(cache_df))\n",
    "\n",
    "# Merge back into Localiza dataset\n",
    "loc_matched = loc_df.merge(cache_df.rename(columns={'brand_norm':'_brand_norm','model_norm':'_model_norm','version_norm':'_version_norm'}), on=['_brand_norm','_model_norm','_version_norm','model_year'], how='left')\n",
    "loc_matched.rename(columns={'score':'match_score'}, inplace=True)\n",
    "loc_matched['match_score'] = pd.to_numeric(loc_matched['match_score'], errors='coerce').fillna(0.0)\n",
    "loc_matched['match_accepted'] = (loc_matched['match_score'] >= THRESHOLD).astype(int)\n",
    "match_out = MATCH_DIR / f\"localiza_with_fipe_match_{ymd_compact()}.csv\"\n",
    "loc_matched.to_csv(match_out, index=False, sep=';')\n",
    "log.info('Saved matched Localiza CSV: %s', match_out)\n",
    "loc_matched[['brand','model','version','fipe_code','match_score','match_accepted']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae959f",
   "metadata": {},
   "source": [
    "## 14. Extract (fipe_code, model_year) Tuples from Localiza & Movida\n",
    "Collect unique pairs actually observed in vendor inventories. These drive:\n",
    "- Which FIPE price rows we keep in time series (filter to what vendors sell)\n",
    "- Audit / coverage monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuples(loc: pd.DataFrame, mov: pd.DataFrame):\n",
    "    out = set()\n",
    "    if not loc.empty and 'fipe_code' in loc.columns:\n",
    "        for _, r in loc[['fipe_code','model_year']].dropna().iterrows():\n",
    "            code = str(r['fipe_code']).strip();\n",
    "            try: yr = int(r['model_year'])\n",
    "            except: continue\n",
    "            if code:\n",
    "                out.add((code, yr))\n",
    "    if not mov.empty and 'fipe_code' in mov.columns:\n",
    "        for _, r in mov[['fipe_code','model_year']].dropna().iterrows():\n",
    "            code = str(r['fipe_code']).strip();\n",
    "            try: yr = int(r['model_year'])\n",
    "            except: continue\n",
    "            if code:\n",
    "                out.add((code, yr))\n",
    "    return out\n",
    "\n",
    "tuples_set = collect_tuples(loc_matched, mov_df)\n",
    "log.info('Unique tuples collected: %d', len(tuples_set))\n",
    "list(sorted(list(tuples_set))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf13170",
   "metadata": {},
   "source": [
    "## 15. Generate Tuples Audit DataFrame & Export\n",
    "Creates a small table counting how many rows contributed each (fipe_code, model_year) per vendor.\n",
    "Use this to see which models dominate stock. Saved to `data/tuples/` with today’s date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2583647",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = loc_matched[['fipe_code','model_year']].dropna().copy(); ldf['fipe_code']=ldf['fipe_code'].astype(str).str.strip()\n",
    "mdf = mov_df[['fipe_code','model_year']].dropna().copy() if ('fipe_code' in mov_df.columns) else pd.DataFrame(columns=['fipe_code','model_year'])\n",
    "if not mdf.empty:\n",
    "    mdf['fipe_code']=mdf['fipe_code'].astype(str).str.strip()\n",
    "\n",
    "lcnt = (ldf.groupby(['fipe_code','model_year']).size().rename('localiza_count').reset_index()) if not ldf.empty else pd.DataFrame(columns=['fipe_code','model_year','localiza_count'])\n",
    "cnt2 = (mdf.groupby(['fipe_code','model_year']).size().rename('movida_count').reset_index()) if not mdf.empty else pd.DataFrame(columns=['fipe_code','model_year','movida_count'])\n",
    "audit = pd.merge(lcnt, cnt2, on=['fipe_code','model_year'], how='outer')\n",
    "for c in ['localiza_count','movida_count']:\n",
    "    if c not in audit.columns: audit[c]=0\n",
    "audit[['localiza_count','movida_count']] = audit[['localiza_count','movida_count']].fillna(0).astype(int)\n",
    "audit['total_count'] = audit['localiza_count'] + audit['movida_count']\n",
    "audit['localiza_present'] = audit['localiza_count'] > 0\n",
    "audit['movida_present'] = audit['movida_count'] > 0\n",
    "TUPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "audit_out = TUPLES_DIR / f'fipe_tuples_{ymd_compact()}.csv'\n",
    "audit.to_csv(audit_out, index=False, sep=';')\n",
    "log.info('Tuples audit saved (canonical schema): %s (rows=%d)', audit_out, len(audit))\n",
    "audit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b7f61",
   "metadata": {},
   "source": [
    "## 16. Load Existing FIPE Dump\n",
    "A richer FIPE dump with pricing (`ValorNum`, `MesReferencia`). If missing we still finish the pipeline, but price enrichment & time series become empty.\n",
    "\n",
    "The file pattern: `data/fipe/fipe_dump_YYYYMMDD.csv` (we pick the newest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285df96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fipe_dump_csv is None:\n",
    "    log.warning('No FIPE dump CSV found (data/fipe/fipe_dump_*.csv). Skip pricing merge steps if absent.')\n",
    "    fipe_dump = pd.DataFrame()\n",
    "else:\n",
    "    fipe_dump = pd.read_csv(fipe_dump_csv)\n",
    "    # Convert price column to numeric if present (drops formatting issues)\n",
    "    if 'ValorNum' in fipe_dump.columns:\n",
    "        fipe_dump['ValorNum'] = pd.to_numeric(fipe_dump['ValorNum'], errors='coerce')\n",
    "log.info('FIPE dump rows: %d', len(fipe_dump))\n",
    "fipe_dump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b6752",
   "metadata": {},
   "source": [
    "## 17. Build Localiza Vendor Table (Merge FIPE Pricing)\n",
    "Joins Localiza rows with FIPE price for the same (fipe_code, model_year, snapshot_year, snapshot_month) so the price aligns with the snapshot month.\n",
    "\n",
    "Additional rules:\n",
    "- fipe_version is a deterministic mapping of fipe_code only (independent of model_year).\n",
    "- SiglaCombustivel from FIPE dump is merged by fipe_code; if SiglaCombustivel in {E, H} we force type = EV.\n",
    "\n",
    "Implementation details:\n",
    "- If `MesReferencia` exists in FIPE dump we parse and shift (publish month -> market month = month-1) and match on (year, month).\n",
    "- If `MesReferencia` is absent we fall back to code + model_year only (coarser price match).\n",
    "- `premium_vs_fipe_price` = (offer_price – fipe_price) / fipe_price.\n",
    "\n",
    "If no FIPE price data available the new columns will be blank (NA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabef2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical vendor table builder with row-count preservation (schema aligned with fleet.py build_vendor_table)\n",
    "# Rules:\n",
    "# - fipe_version derives ONLY from fipe_code (stable mapping from FIPE dump; year/month independent)\n",
    "# - fipe_price chosen by (fipe_code, model_year, snapshot_year, snapshot_month) when MesReferencia exists, else by (fipe_code, model_year)\n",
    "# - type normalization enhanced: merge SiglaCombustivel from FIPE dump by fipe_code. If SiglaCombustivel in {E,H} => type 'EV'\n",
    "# Output columns: snapshot_date,snapshot_year,snapshot_month,type,brand,model,\n",
    "# fipe_version,fipe_code,manufacture_year,model_year,offer_price,fipe_price,\n",
    "# premium_vs_fipe_price,fipe_code_model_year,model_model_year\n",
    "\n",
    "def build_vendor_table_offline(df: pd.DataFrame, fipe_dump: pd.DataFrame, vendor: str) -> pd.DataFrame:\n",
    "    cols_schema = [\n",
    "        'snapshot_date','snapshot_year','snapshot_month','type','brand','model','fipe_version','fipe_code',\n",
    "        'manufacture_year','model_year','offer_price','fipe_price','premium_vs_fipe_price',\n",
    "        'fipe_code_model_year','model_model_year'\n",
    "    ]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=cols_schema)\n",
    "    work = df.copy()\n",
    "    orig_len = len(work)\n",
    "    work['_orig_row_id'] = range(orig_len)\n",
    "    # Ensure required columns exist\n",
    "    for c in ['snapshot_date','type','brand','model','manufacture_year','model_year','price','fipe_code']:\n",
    "        if c not in work.columns:\n",
    "            work[c] = pd.NA\n",
    "\n",
    "    # Snapshot date decomposition\n",
    "    work['snapshot_date'] = pd.to_datetime(work['snapshot_date'], errors='coerce')\n",
    "    work['snapshot_year'] = work['snapshot_date'].dt.year\n",
    "    work['snapshot_month'] = work['snapshot_date'].dt.month\n",
    "    work['snapshot_date'] = work['snapshot_date'].dt.date.astype(str)\n",
    "\n",
    "    # Build stable fipe_code -> fipe_version mapping (deterministic)\n",
    "    fipe_version_map = {}\n",
    "    if not fipe_dump.empty and {'CodigoFipe','Modelo'}.issubset(fipe_dump.columns):\n",
    "        fv = (fipe_dump[['CodigoFipe','Modelo']]\n",
    "                .dropna(subset=['CodigoFipe','Modelo'])\n",
    "                .rename(columns={'CodigoFipe':'fipe_code','Modelo':'fipe_version'}))\n",
    "        fv['len'] = fv['fipe_version'].str.len()\n",
    "        fv = (fv.sort_values(['fipe_code','len','fipe_version'])\n",
    "                .drop_duplicates(subset=['fipe_code']))\n",
    "        fipe_version_map = dict(zip(fv['fipe_code'].astype(str), fv['fipe_version']))\n",
    "    # Assign fipe_version purely from mapping\n",
    "    work['fipe_version'] = work['fipe_code'].astype(str).map(fipe_version_map)\n",
    "\n",
    "    # Helper to parse MesReferencia\n",
    "    def _parse_mes_label(label: str):\n",
    "        pt = {'janeiro':1,'fevereiro':2,'março':3,'marco':3,'abril':4,'maio':5,'junho':6,'julho':7,'agosto':8,'setembro':9,'outubro':10,'novembro':11,'dezembro':12}\n",
    "        s = (label or '').lower().replace('\\xa0',' ').strip()\n",
    "        s = re.sub(r'\\s+',' ', s)\n",
    "        s_norm = re.sub(r'[/-]',' ', s)\n",
    "        m = re.search(r'(janeiro|fevereiro|março|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\s+(?:de\\s+)?(\\d{4})', s_norm)\n",
    "        if m: return int(m.group(2)), pt[m.group(1)]\n",
    "        m2 = re.search(r'(1[0-2]|0?[1-9])\\s+(\\d{4})', s_norm)\n",
    "        if m2: return int(m2.group(2)), int(m2.group(1))\n",
    "        return 1900,1\n",
    "\n",
    "    # Merge FIPE price\n",
    "    if not fipe_dump.empty and {'CodigoFipe','AnoModelo'}.issubset(fipe_dump.columns):\n",
    "        fipe_sub = fipe_dump.copy()\n",
    "        has_mes = 'MesReferencia' in fipe_sub.columns\n",
    "        if has_mes:\n",
    "            fipe_sub[['reference_year','reference_month']] = fipe_sub['MesReferencia'].map(_parse_mes_label).to_list()\n",
    "            def _shift(y,m): return (y-1,12) if m==1 else (y,m-1)\n",
    "            fipe_sub[['reference_year','reference_month']] = fipe_sub.apply(lambda r: _shift(int(r['reference_year']), int(r['reference_month'])), axis=1, result_type='expand')\n",
    "        fipe_sub = fipe_sub.rename(columns={'CodigoFipe':'fipe_code','AnoModelo':'model_year','ValorNum':'fipe_price'})\n",
    "        fipe_sub['fipe_price'] = pd.to_numeric(fipe_sub.get('fipe_price'), errors='coerce')\n",
    "        base_cols = ['fipe_code','model_year','fipe_price'] + (['reference_year','reference_month'] if has_mes else [])\n",
    "        fipe_sub = fipe_sub[[c for c in base_cols if c in fipe_sub.columns]].dropna(subset=['fipe_code','model_year'])\n",
    "        if has_mes:\n",
    "            fipe_sub = (fipe_sub\n",
    "                        .sort_values(['fipe_code','model_year','reference_year','reference_month'])\n",
    "                        .drop_duplicates(subset=['fipe_code','model_year','reference_year','reference_month'], keep='last'))\n",
    "            work = work.merge(\n",
    "                fipe_sub,\n",
    "                left_on=['fipe_code','model_year','snapshot_year','snapshot_month'],\n",
    "                right_on=['fipe_code','model_year','reference_year','reference_month'],\n",
    "                how='left'\n",
    "            )\n",
    "            work.drop(columns=[c for c in ['reference_year','reference_month'] if c in work.columns], inplace=True)\n",
    "        else:\n",
    "            work = work.merge(fipe_sub, on=['fipe_code','model_year'], how='left')\n",
    "        if len(work) != orig_len:\n",
    "            work = (work.sort_values(['_orig_row_id'])\n",
    "                        .groupby('_orig_row_id', as_index=False)\n",
    "                        .first())\n",
    "    else:\n",
    "        if 'fipe_price' not in work.columns: work['fipe_price'] = pd.NA\n",
    "\n",
    "    # Merge SiglaCombustivel by fipe_code (most frequent value per code)\n",
    "    if not fipe_dump.empty and {'CodigoFipe','SiglaCombustivel'}.issubset(fipe_dump.columns):\n",
    "        sig = (fipe_dump[['CodigoFipe','SiglaCombustivel']]\n",
    "                 .dropna(subset=['CodigoFipe','SiglaCombustivel'])\n",
    "                 .rename(columns={'CodigoFipe':'fipe_code'}))\n",
    "        # choose most frequent per code\n",
    "        sig = (sig.groupby('fipe_code')['SiglaCombustivel']\n",
    "                 .agg(lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0])\n",
    "                 .reset_index())\n",
    "        work = work.merge(sig, on='fipe_code', how='left')\n",
    "\n",
    "    # Normalize type incorporating SiglaCombustivel; E or H => EV\n",
    "    base_type = work.apply(lambda r: normalize_type(r.get('type'), r.get('SiglaCombustivel')), axis=1)\n",
    "    work['type'] = np.where(work['SiglaCombustivel'].isin(['E','H']), 'EV', base_type)\n",
    "\n",
    "    work['offer_price'] = work.get('price')\n",
    "    work['fipe_price'] = pd.to_numeric(work.get('fipe_price'), errors='coerce')\n",
    "    work['premium_vs_fipe_price'] = np.where(work['fipe_price'].gt(0), (work['offer_price'] - work['fipe_price'])/work['fipe_price'], pd.NA)\n",
    "    work['fipe_code_model_year'] = work['fipe_code'].astype(str) + '_' + work['model_year'].astype(str)\n",
    "    work['model_model_year'] = work['model'].astype(str) + '_' + work['model_year'].astype(str)\n",
    "\n",
    "    for c in cols_schema:\n",
    "        if c not in work.columns:\n",
    "            work[c] = pd.NA\n",
    "\n",
    "    vendor_tbl = work[cols_schema].copy()\n",
    "    if len(vendor_tbl) != orig_len:\n",
    "        log.warning('%s vendor table row count mismatch (orig=%s, out=%s)', vendor, orig_len, len(vendor_tbl))\n",
    "\n",
    "    # Diagnostics: confirm uniqueness of mapping\n",
    "    if not vendor_tbl['fipe_version'].isna().all():\n",
    "        inconsistent = vendor_tbl.groupby('fipe_code')['fipe_version'].nunique().gt(1)\n",
    "        if inconsistent.any():\n",
    "            log.warning('Inconsistent fipe_version mapping for some fipe_code.')\n",
    "    return vendor_tbl\n",
    "\n",
    "loc_vendor = build_vendor_table_offline(loc_matched, fipe_dump, 'localiza')\n",
    "\n",
    "# Harmonize Onix Sedan Plus naming in Localiza vendor table\n",
    "mask = (\n",
    "    loc_vendor['fipe_version'].fillna('').str.lower().str.startswith('onix sedan plus') &\n",
    "    loc_vendor['model'].fillna('').str.lower().eq('onix')\n",
    ")\n",
    "changed = mask.sum()\n",
    "loc_vendor.loc[mask, 'fipe_version'] = 'onix plus'\n",
    "print('Adjusted fipe_version to \"onix plus\" for rows:', changed)\n",
    "\n",
    "loc_vendor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f9037",
   "metadata": {},
   "source": [
    "## 18. Build Movida Vendor Table (Merge FIPE Pricing)\n",
    "Same enrichment for Movida rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fa947",
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_vendor = build_vendor_table_offline(mov_df, fipe_dump, 'movida')\n",
    "# Validation: fipe_version uniqueness per fipe_code across both vendors\n",
    "combined = pd.concat([loc_vendor[['fipe_code','fipe_version']], mov_vendor[['fipe_code','fipe_version']]])\n",
    "uniques = combined.groupby('fipe_code')['fipe_version'].nunique()\n",
    "violations = uniques[uniques>1]\n",
    "print('Unique fipe_code values:', combined['fipe_code'].nunique())\n",
    "print('fipe_code with >1 fipe_version (should be 0):', len(violations))\n",
    "if len(violations):\n",
    "    print('Violating codes:', violations.index.tolist()[:20])\n",
    "\n",
    "mov_vendor.loc[mov_vendor['model'].str.lower()=='commander','type'] = 'suv'\n",
    "mov_vendor.loc[mov_vendor['fipe_code'].str.lower()=='600005','fipe_code'] = '022189-9'\n",
    "mov_vendor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088368f0",
   "metadata": {},
   "source": [
    "## 19. Build Consolidated FIPE Time Series Table\n",
    "From the FIPE dump we build a monthly time series only for (fipe_code, model_year) pairs present in vendor stock.\n",
    "\n",
    "Important details:\n",
    "- FIPE month label refers to *publication month*; we shift one month back so price becomes valid for the actual market usage window.\n",
    "- We compute `m_m_price_change` (percentage change vs previous month).\n",
    "\n",
    "If no FIPE dump is present this table will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mes_label(label: str):\n",
    "    \"\"\"Parse Portuguese month label (e.g. 'Julho de 2025') into (year, month).\"\"\"\n",
    "    pt = {'janeiro':1,'fevereiro':2,'março':3,'marco':3,'abril':4,'maio':5,'junho':6,'julho':7,'agosto':8,'setembro':9,'outubro':10,'novembro':11,'dezembro':12}\n",
    "    s = (label or '').lower().replace('\\xa0',' ').strip()\n",
    "    s = re.sub(r'\\s+',' ', s)\n",
    "    s_norm = re.sub(r'[/-]',' ', s)\n",
    "    m = re.search(r'(janeiro|fevereiro|março|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\s+(?:de\\s+)?(\\d{4})', s_norm)\n",
    "    if m:\n",
    "        return int(m.group(2)), pt[m.group(1)]\n",
    "    m2 = re.search(r'(1[0-2]|0?[1-9])\\s+(\\d{4})', s_norm)\n",
    "    if m2:\n",
    "        return int(m2.group(2)), int(m2.group(1))\n",
    "    return 1900,1  # Fallback improbable sentinel\n",
    "\n",
    "if fipe_dump.empty:\n",
    "    fipe_series = pd.DataFrame(columns=['reference_year','reference_month','brand','model','type','fipe_version','fipe_code','model_year','fipe_price','m_m_price_change'])\n",
    "else:\n",
    "    fdf = fipe_dump.copy()\n",
    "    if 'MesReferencia' not in fdf.columns:\n",
    "        log.warning('Missing MesReferencia in FIPE dump; cannot build time series')\n",
    "        fipe_series = pd.DataFrame(columns=['reference_year','reference_month','brand','model','type','fipe_version','fipe_code','model_year','fipe_price','m_m_price_change'])\n",
    "    else:\n",
    "        fdf['reference_year'], fdf['reference_month'] = zip(*fdf['MesReferencia'].map(parse_mes_label))\n",
    "        # Shift back one month (publish M -> market M-1)\n",
    "        def _shift(y,m): return (y-1,12) if m==1 else (y, m-1)\n",
    "        fdf['reference_year'], fdf['reference_month'] = zip(*fdf.apply(lambda r: _shift(int(r['reference_year']), int(r['reference_month'])), axis=1))\n",
    "        fdf = fdf.rename(columns={'CodigoFipe':'fipe_code','AnoModelo':'model_year','ValorNum':'fipe_price','Marca':'brand','Modelo':'fipe_version'})\n",
    "        # Map model presence from vendor tables\n",
    "        model_map = pd.concat([\n",
    "            loc_vendor[['fipe_code','model_year','model']],\n",
    "            mov_vendor[['fipe_code','model_year','model']]\n",
    "        ], ignore_index=True).dropna().drop_duplicates(subset=['fipe_code','model_year'])\n",
    "        fdf = fdf.merge(model_map, on=['fipe_code','model_year'], how='left')\n",
    "        # Keep only if model present in vendor fleet\n",
    "        fdf = fdf[fdf['model'].notna()].copy()\n",
    "        # Merge type preference (loc > mov) using already unified vendor tables\n",
    "        try:\n",
    "            type_map = pd.concat([\n",
    "                loc_vendor[['fipe_code','model_year','type']].assign(_pref=0),\n",
    "                mov_vendor[['fipe_code','model_year','type']].assign(_pref=1)\n",
    "            ], ignore_index=True).dropna(subset=['type'])\n",
    "            type_map = type_map.sort_values(['fipe_code','model_year','_pref']).drop_duplicates(subset=['fipe_code','model_year'])[['fipe_code','model_year','type']]\n",
    "            fdf = fdf.merge(type_map, on=['fipe_code','model_year'], how='left')\n",
    "        except Exception as e:\n",
    "            log.warning('Type merge skipped: %s', e)\n",
    "        fdf = fdf.sort_values(['fipe_code','model_year','reference_year','reference_month'])\n",
    "        fdf = fdf.drop_duplicates(subset=['fipe_code','model_year','reference_year','reference_month'], keep='last')\n",
    "        fdf['m_m_price_change'] = fdf.groupby(['fipe_code','model_year'])['fipe_price'].pct_change()\n",
    "        fipe_series = fdf[['reference_year','reference_month','brand','model','type','fipe_version','fipe_code','model_year','fipe_price','m_m_price_change']]\n",
    "fipe_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8397c",
   "metadata": {},
   "source": [
    "## 20. Compute Price Premium & Month-over-Month Changes\n",
    "Nothing to run here: metrics are already produced in earlier steps (`premium_vs_fipe_price` in Sections 17–18, `m_m_price_change` in Section 19). Section kept for narrative completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae56c29",
   "metadata": {},
   "source": [
    "## 21. Finalize & Export All Output Tables\n",
    "Writes dated CSVs so historical runs are preserved. You can safely re-run the entire notebook any day; new files will have a new date stamp.\n",
    "\n",
    "Next steps after export:\n",
    "- Update tables in \"Car Price Index\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55598fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = ymd_compact()\n",
    "ensure_dir(TABLES_DIR)\n",
    "loc_vendor_out = TABLES_DIR / 'localiza' / f'localiza_table_{STAMP}.csv'\n",
    "mov_vendor_out = TABLES_DIR / 'movida' / f'movida_table_{STAMP}.csv'\n",
    "fipe_series_out = TABLES_DIR / 'fipe' / f'fipe_table_{STAMP}.csv'\n",
    "\n",
    "# Row count preservation checks\n",
    "raw_loc_len = len(loc_matched) if 'loc_matched' in globals() else len(loc_df)\n",
    "raw_mov_len = len(mov_df)\n",
    "if len(loc_vendor) != raw_loc_len:\n",
    "    log.warning('Localiza vendor table rows (%s) differ from raw matched rows (%s)', len(loc_vendor), raw_loc_len)\n",
    "if len(mov_vendor) != raw_mov_len:\n",
    "    log.warning('Movida vendor table rows (%s) differ from raw rows (%s)', len(mov_vendor), raw_mov_len)\n",
    "\n",
    "loc_vendor.to_csv(loc_vendor_out, index=False, sep=';')\n",
    "mov_vendor.to_csv(mov_vendor_out, index=False, sep=';')\n",
    "if not fipe_series.empty:\n",
    "    fipe_series.to_csv(fipe_series_out, index=False, sep=';')\n",
    "log.info('Exported canonical vendor tables and FIPE series (loc=%s rows, mov=%s rows).', len(loc_vendor), len(mov_vendor))\n",
    "loc_vendor.head(), mov_vendor.head(), fipe_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d7e37",
   "metadata": {},
   "source": [
    "---\n",
    "### Quick Troubleshooting\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|--------|--------------|-----|\n",
    "| FileNotFound error for Localiza | No CSV in `raw/localiza/` | Put a file there and re-run from Section 2 |\n",
    "| Zero new matches | Cache already covers all version keys | Normal; proceed |\n",
    "| Many low scores (< threshold) | New trims / abbreviations | Add mapping inside normalization (Section 4) |\n",
    "| Empty FIPE series table | No FIPE dump file found | Add a `fipe_dump_*.csv` to `data/fipe/` |\n",
    "| premium_vs_fipe_price all blank | No price column or no FIPE price | Check raw file or add FIPE dump |\n",
    "\n",
    "If stuck, copy the cell output and ask copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433dc813",
   "metadata": {},
   "source": [
    "## 22. Exploratory Analysis & Visualizations\n",
    "Quick, optional data exploration for business users. Just run the cells below after exports.\n",
    "\n",
    "If a plot cell shows a warning about a missing library (e.g. Plotly), you can still see static Matplotlib charts.\n",
    "\n",
    "Skip any cell if underlying data is empty. Each cell is defensive and will tell you if it had nothing to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization setup (uses matplotlib; tries plotly if installed)\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    _HAS_PLOTLY = True\n",
    "except Exception:\n",
    "    _HAS_PLOTLY = False\n",
    "\n",
    "# Helper to show top N rows with safe fallback\n",
    "def show_top(df, n=5):\n",
    "    return df.head(n) if not df.empty else pd.DataFrame()\n",
    "\n",
    "print('Plotly available:' , _HAS_PLOTLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8d13b",
   "metadata": {},
   "source": [
    "### 22.1 Stock Composition by Body Type\n",
    "Shows distribution of normalized type (if derivable) or approximated via version text tokens. Useful to see fleet mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151141e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock composition using canonical vendor table (loc_vendor)\n",
    "# 'type' already normalized; fallback quick inference only if missing.\n",
    "import re\n",
    "\n",
    "def infer_type_from_model_row(r):\n",
    "    s = (r.get('fipe_version') or r.get('model') or '').lower()\n",
    "    if any(k in s for k in ['suv','compass','tracker','t cross','t-cross','renegade','hr-v','kicks','nivus']): return 'suv'\n",
    "    if any(k in s for k in ['hatch','onix','gol','fox','hb20 ','argo','mobi','kwid']): return 'hatch'\n",
    "    if any(k in s for k in ['sedan','virtus','voyage','corolla','cruze','civic','onix sedan','hb20s']): return 'sedan'\n",
    "    if any(k in s for k in ['strada','s10','hilux','oro ch','oroch','ranger','amarok','l200','frontier']): return 'pickup'\n",
    "    return 'other'\n",
    "\n",
    "if 'loc_vendor' not in globals() or loc_vendor.empty:\n",
    "    print('No Localiza vendor table available for stock composition.')\n",
    "else:\n",
    "    stock = loc_vendor.copy()\n",
    "    if 'type' not in stock.columns or stock['type'].isna().all():\n",
    "        stock['type'] = stock.apply(infer_type_from_model_row, axis=1)\n",
    "    ct = stock['type'].fillna('unknown').value_counts().rename_axis('type').reset_index(name='count')\n",
    "    if ct.empty:\n",
    "        print('No data to plot.')\n",
    "    else:\n",
    "        if _HAS_PLOTLY:\n",
    "            fig = px.bar(ct, x='type', y='count', title='Stock Composition (Localiza)', text='count')\n",
    "            fig.update_traces(textposition='outside')\n",
    "            fig.show()\n",
    "        else:\n",
    "            ax = ct.plot(kind='bar', x='type', y='count', legend=False, title='Stock Composition (Localiza)')\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(int(p.get_height()), (p.get_x() + p.get_width()/2, p.get_height()), ha='center', va='bottom', fontsize=9)\n",
    "            plt.show()\n",
    "    ct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67d08a",
   "metadata": {},
   "source": [
    "### 22.2 Price Premium Distribution\n",
    "Shows how Localiza asking prices compare vs FIPE reference (only matched rows with FIPE price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8733ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prem = loc_vendor.dropna(subset=['premium_vs_fipe_price','fipe_price'])\n",
    "if prem.empty:\n",
    "    print('No matched rows with FIPE price to analyze premium.')\n",
    "else:\n",
    "    prem['premium_pct'] = prem['premium_vs_fipe_price'] * 100\n",
    "    if _HAS_PLOTLY:\n",
    "        fig = px.histogram(prem, x='premium_pct', nbins=40, title='Price Premium vs FIPE (%)', marginal='box')\n",
    "        fig.add_vline(x=0, line_dash='dash', line_color='black')\n",
    "        fig.show()\n",
    "    else:\n",
    "        ax = prem['premium_pct'].plot(kind='hist', bins=40, title='Price Premium vs FIPE (%)')\n",
    "        ax.axvline(0, color='black', linestyle='--')\n",
    "        plt.xlabel('% Premium')\n",
    "        plt.show()\n",
    "    # Display sample rows (vendor schema has no raw version; show fipe_version instead)\n",
    "    prem[['brand','model','fipe_version','premium_pct']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113ba92",
   "metadata": {},
   "source": [
    "### 22.3 Top Models by Stock & Premium\n",
    "Table of models with highest presence and their median premium vs FIPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_base = loc_vendor.copy() if 'loc_vendor' in globals() else pd.DataFrame()\n",
    "needed = {'brand','model','fipe_code','offer_price'}\n",
    "if top_base.empty or not needed.issubset(top_base.columns):\n",
    "    print('Not enough data for top models table.')\n",
    "else:\n",
    "    # Keep only rows with an offer price\n",
    "    top_base = top_base[top_base['offer_price'].notna()]\n",
    "    if top_base.empty:\n",
    "        print('No non-null offer_price values available for aggregation.')\n",
    "    else:\n",
    "        # Compute aggregations; median skips NaN by default\n",
    "        grp = (top_base.groupby(['brand','model'])\n",
    "               .agg(stock=('fipe_code','count'),\n",
    "                    avg_offer_price=('offer_price', lambda s: float(np.nanmean(s)) if s.notna().any() else np.nan),\n",
    "                    median_premium=('premium_vs_fipe_price', lambda s: float(np.nanmedian(s)) if s.notna().any() else np.nan))\n",
    "               .reset_index())\n",
    "        # Drop groups with no price info\n",
    "        grp = grp[grp['avg_offer_price'].notna()]\n",
    "        if grp.empty:\n",
    "            print('All groups lacked valid offer_price after filtering.')\n",
    "        else:\n",
    "            grp = grp.sort_values('stock', ascending=False)\n",
    "            display_cols = ['brand','model','stock','avg_offer_price','median_premium']\n",
    "            grp['avg_offer_price'] = grp['avg_offer_price'].round(0).astype(int)\n",
    "            print('Top 15 models by stock (filtered for valid prices):')\n",
    "            display(grp.head(15)[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd81acc",
   "metadata": {},
   "source": [
    "### 22.4 Time Series: Example FIPE Price Trend\n",
    "Pick a (fipe_code, model_year) with enough history to view month-over-month price evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fipe_series' not in globals() or fipe_series.empty:\n",
    "    print('No FIPE series data loaded.')\n",
    "else:\n",
    "    freq = (fipe_series.groupby(['fipe_code','model_year'])\n",
    "            .size().rename('points').reset_index().sort_values('points', ascending=False))\n",
    "    if freq.empty:\n",
    "        print('No time series points to plot.')\n",
    "    else:\n",
    "        fc, my = freq.iloc[0][['fipe_code','model_year']]\n",
    "        sel = fipe_series[(fipe_series['fipe_code']==fc) & (fipe_series['model_year']==my)].copy()\n",
    "        sel['date'] = pd.to_datetime(sel['reference_year'].astype(int).astype(str) + '-' + sel['reference_month'].astype(int).astype(str) + '-01')\n",
    "        sel = sel.sort_values('date')\n",
    "        if _HAS_PLOTLY:\n",
    "            fig = px.line(sel, x='date', y='fipe_price', title=f'FIPE Price Trend (code={fc}, year={my})')\n",
    "            fig.update_traces(mode='lines+markers')\n",
    "            fig.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(6,3))\n",
    "            plt.plot(sel['date'], sel['fipe_price'], marker='o')\n",
    "            plt.title(f'FIPE Price Trend (code={fc}, year={my})')\n",
    "            plt.xlabel('Date'); plt.ylabel('Price')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        sel[['date','fipe_price','m_m_price_change']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2162c7e",
   "metadata": {},
   "source": [
    "### 22.5 Match Score Quality Bands\n",
    "Understand distribution of match scores to monitor normalization / threshold health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match score distribution now must read from cache_df or loc_matched (pre-vendor collapse) since vendor tables lack match_score.\n",
    "score_source = 'loc_matched' if 'loc_matched' in globals() else None\n",
    "if not score_source:\n",
    "    print('loc_matched not available for score distribution.')\n",
    "else:\n",
    "    scores_df = loc_matched[['match_score']].dropna()\n",
    "    if scores_df.empty:\n",
    "        print('No match scores to display.')\n",
    "    else:\n",
    "        bins = [0,0.4,0.5,0.6,0.7,0.8,0.9,1.01]\n",
    "        labels = ['<0.4','0.4-0.5','0.5-0.6','0.6-0.7','0.7-0.8','0.8-0.9','0.9+']\n",
    "        scores_df['band'] = pd.cut(scores_df['match_score'], bins=bins, labels=labels, include_lowest=True, right=False)\n",
    "        dist = scores_df['band'].value_counts().reindex(labels).fillna(0).reset_index()\n",
    "        dist.columns = ['band','count']\n",
    "        if _HAS_PLOTLY:\n",
    "            fig = px.bar(dist, x='band', y='count', title='Match Score Distribution', text='count')\n",
    "            fig.update_traces(textposition='outside')\n",
    "            fig.add_vline(x=labels.index('0.6-0.7'), line_color='orange', line_dash='dash')\n",
    "            fig.show()\n",
    "        else:\n",
    "            ax = dist.plot(kind='bar', x='band', y='count', legend=False, title='Match Score Distribution')\n",
    "            for p in ax.patches:\n",
    "                ax.annotate(int(p.get_height()), (p.get_x()+p.get_width()/2, p.get_height()), ha='center', va='bottom')\n",
    "            plt.show()\n",
    "        dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c86f1",
   "metadata": {},
   "source": [
    "### 22.6 Notes & Next Ideas\n",
    "- Add side-by-side comparison for Movida vs Localiza premiums.\n",
    "- Track how match score distribution shifts after updating normalization rules.\n",
    "- Export these plots automatically to an `outputs/` folder for sharing.\n",
    "\n",
    "End of notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
